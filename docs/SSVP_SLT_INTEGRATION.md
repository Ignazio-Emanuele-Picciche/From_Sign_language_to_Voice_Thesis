# ðŸŽ¯ SSVP-SLT Integration Guide

Guida all'integrazione del modello SSVP-SLT di Facebook Research nella pipeline EmoSign.

---

## ðŸ“Œ Overview

Abbiamo integrato **SSVP-SLT** (Self-Supervised Video Pretraining for Sign Language Translation) come modello alternativo/complementare al Seq2Seq Transformer per la traduzione da ASL a testo.

### PerchÃ© SSVP-SLT?

| Caratteristica       | Seq2Seq Transformer (Nostro) | SSVP-SLT (Facebook)      |
| -------------------- | ---------------------------- | ------------------------ |
| **Approccio**        | Landmarks-based              | Video-based (end-to-end) |
| **Pretraining**      | âŒ From scratch              | âœ… Self-supervised (MAE) |
| **BLEU-4**           | ~25-30% (target)             | **38-40%** (SOTA)        |
| **Robustezza**       | Dipende da landmarks         | PiÃ¹ robusto              |
| **InterpretabilitÃ ** | âœ… High (pose/hands/face)    | âš ï¸ Black-box             |
| **Efficienza**       | âœ… PiÃ¹ veloce (35 fps)       | âš ï¸ PiÃ¹ lento (12 fps)    |
| **Memoria**          | âœ… 4GB VRAM                  | âŒ 8-16GB VRAM           |

---

## ðŸ“‚ Struttura Integrazione

```
src/sign_to_text_ssvp/           # Nuovo modulo SSVP-SLT
â”œâ”€â”€ README.md                     # Documentazione completa
â”œâ”€â”€ __init__.py
â”‚
â”œâ”€â”€ configs/                      # Configurazioni fine-tuning
â”‚   â”œâ”€â”€ finetune_quick.yaml      # Test rapido (3 epochs)
â”‚   â”œâ”€â”€ finetune_base.yaml       # Full training (30 epochs)
â”‚   â””â”€â”€ finetune_large.yaml      # Large model (max performance)
â”‚
â”œâ”€â”€ scripts/                      # Script helper
â”‚   â”œâ”€â”€ install_ssvp.sh          # Installazione automatica
â”‚   â””â”€â”€ prepare_all_splits.sh    # Preparazione dataset
â”‚
â”œâ”€â”€ models/                       # Modelli e checkpoints
â”‚   â”œâ”€â”€ checkpoints/             # Pretrained models
â”‚   â””â”€â”€ ssvp_slt_repo/           # Clone repository SSVP-SLT
â”‚
â”œâ”€â”€ docs/                         # Documentazione
â”‚   â””â”€â”€ QUICKSTART.md            # Quick start guide
â”‚
â””â”€â”€ Python scripts:
    â”œâ”€â”€ download_pretrained.py    # Download modelli pretrained
    â”œâ”€â”€ prepare_how2sign_for_ssvp.py  # Conversione dataset
    â”œâ”€â”€ finetune_how2sign.py      # Fine-tuning (placeholder)
    â”œâ”€â”€ evaluate_how2sign.py      # Evaluation (placeholder)
    â””â”€â”€ compare_models.py         # Comparazione modelli (placeholder)
```

---

## ðŸš€ Quick Start

### 1. Installazione (5 minuti)

```bash
cd src/sign_to_text_ssvp
bash scripts/install_ssvp.sh
```

### 2. Download Modello Pretrained

```bash
python download_pretrained.py --model base
```

### 3. Preparazione Dataset

```bash
bash scripts/prepare_all_splits.sh
```

### 4. Fine-tuning

```bash
# Quick test
python finetune_how2sign.py --config configs/finetune_quick.yaml

# Full training
python finetune_how2sign.py --config configs/finetune_base.yaml
```

**Nota**: Gli script Python di fine-tuning, evaluation e comparison sono placeholder che richiedono implementazione dopo installazione SSVP-SLT.

---

## ðŸŽ¯ Use Cases

### 1. **Benchmark State-of-the-Art**

Usa SSVP-SLT per stabilire upper bound performance:

```python
# Valuta entrambi i modelli
python src/sign_to_text_ssvp/evaluate_how2sign.py --checkpoint ... --split test
python src/sign_to_text/evaluate_how2sign.py --checkpoint ... --split test

# Confronta risultati
python src/sign_to_text_ssvp/compare_models.py --ssvp ... --seq2seq ...
```

### 2. **Production Deployment**

Scegli modello basato su trade-off:

```
Accuracy prioritaria â†’ SSVP-SLT (40% BLEU)
Speed prioritaria â†’ Seq2Seq (35 fps)
InterpretabilitÃ  â†’ Seq2Seq (landmarks)
```

### 3. **Ensemble Model**

Combina predizioni per robustezza:

```python
# Weighted ensemble
final_prediction = 0.6 * ssvp_output + 0.4 * seq2seq_output
```

### 4. **Thesis Contribution**

Mostra nella tesi:

- âœ… Comparison landmarks-based vs video-based
- âœ… Trade-off accuracy vs efficiency
- âœ… Quando usare quale approccio
- âœ… Ablation study: effetto pretraining

---

## ðŸ“Š Performance Target

### Expected Results (dopo fine-tuning su How2Sign)

| Model             | BLEU-4     | BLEU-1 | WER    | CER    | Speed   |
| ----------------- | ---------- | ------ | ------ | ------ | ------- |
| **SSVP-SLT Base** | **38-40%** | 52-55% | 25-30% | 12-16% | 12 fps  |
| Seq2Seq (nostro)  | 25-30%     | 42-45% | 40-50% | 25-30% | 35 fps  |
| **Improvement**   | **+13%**   | +10%   | -15%   | -13%   | -23 fps |

---

## ðŸ”„ Integrazione Pipeline EmoSign

### Pipeline Attuale

```
Video ASL â†’ Landmarks (OpenPose) â†’ Seq2Seq â†’ Text â†’ Emotion â†’ TTS
```

### Pipeline con SSVP-SLT

```
Video ASL â†’ SSVP-SLT â†’ Text â†’ Emotion â†’ TTS
```

### Pipeline Ensemble (proposta)

```
                    â”Œâ”€ Landmarks â†’ Seq2Seq â”€â”
Video ASL â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¤                         â”œâ”€â†’ Text (ensemble) â†’ Emotion â†’ TTS
            â””â”€â”€â”€â”€â”€â”€â†’ SSVP-SLT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Vantaggi Ensemble**:

- Robustezza a failure di un modello
- Combina interpretabilitÃ  + accuracy
- Fallback se landmarks extraction fails

---

## ðŸ“ TODO List

### Implementazione Completa

- [x] Setup directory structure
- [x] Documentazione completa (README.md)
- [x] Script installazione (install_ssvp.sh)
- [x] Script download pretrained (download_pretrained.py)
- [x] Script preparazione dataset (prepare_how2sign_for_ssvp.py)
- [x] File configurazione YAML (3 configs)
- [x] Quick start guide (QUICKSTART.md)
- [ ] **Script fine-tuning** (finetune_how2sign.py) â†’ Richiede studio API SSVP-SLT
- [ ] **Script evaluation** (evaluate_how2sign.py) â†’ Dopo fine-tuning
- [ ] **Script comparison** (compare_models.py) â†’ Dopo entrambi i modelli trained
- [ ] Test fine-tuning completo su How2Sign
- [ ] Evaluation e benchmark vs Seq2Seq
- [ ] Integrazione in pipeline EmoSign

### Ricerca e Tesi

- [ ] Ablation study: effect of pretraining
- [ ] Comparison landmarks vs video approach
- [ ] Error analysis: dove SSVP-SLT > Seq2Seq?
- [ ] Ensemble experiments
- [ ] Write thesis section on model comparison

---

## ðŸ”§ Next Steps

### Step 1: Installazione e Setup (oggi)

```bash
cd src/sign_to_text_ssvp
bash scripts/install_ssvp.sh
python download_pretrained.py --model base
bash scripts/prepare_all_splits.sh
```

### Step 2: Implementazione Fine-tuning (1-2 giorni)

Studiare API SSVP-SLT e implementare `finetune_how2sign.py`:

```bash
# Riferimenti
ls -lh models/ssvp_slt_repo/translation/
cat models/ssvp_slt_repo/translation/README.md
```

### Step 3: Training e Evaluation (1 settimana)

```bash
# Quick test
python finetune_how2sign.py --config configs/finetune_quick.yaml

# Full training
python finetune_how2sign.py --config configs/finetune_base.yaml

# Evaluate
python evaluate_how2sign.py --checkpoint ... --split test
```

### Step 4: Comparison e Tesi (1 settimana)

```bash
# Compare models
python compare_models.py --ssvp ... --seq2seq ...

# Write thesis sections
# - Model comparison
# - Results analysis
# - Discussion
```

---

## ðŸ“š References

1. **SSVP-SLT Paper**: [Rust et al. 2024 - ACL](https://aclanthology.org/2024.acl-long.467/)
2. **Repository**: [facebookresearch/ssvp_slt](https://github.com/facebookresearch/ssvp_slt)
3. **How2Sign Dataset**: [Duarte et al. 2021](https://how2sign.github.io/)
4. **Our Seq2Seq**: `src/sign_to_text/README.md`

---

## ðŸ’¡ Tips

### Per Training Efficiente

- Usa `finetune_quick.yaml` per test rapidi
- Monitora loss: deve scendere sotto 2.0 rapidamente
- Salva checkpoint ogni 5 epoche

### Per Debugging

- Usa `--max_samples 100` per test veloci
- Controlla manifest TSV prima di training
- Verifica video paths con symlink

### Per Performance

- Usa modello Base su V100/A100
- Abilita mixed precision (fp16)
- Batch size 16 per Base, 8 per Large

---

## ðŸ“ž Support

- **Documentazione SSVP-SLT**: `src/sign_to_text_ssvp/README.md`
- **Quick Start**: `src/sign_to_text_ssvp/docs/QUICKSTART.md`
- **SSVP-SLT Issues**: https://github.com/facebookresearch/ssvp_slt/issues
- **Paper**: https://arxiv.org/abs/2402.09611
