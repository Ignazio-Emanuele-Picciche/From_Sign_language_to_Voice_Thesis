#!/usr/bin/env python3
"""
SONAR Fine-Tuning Script (APPROCCIO CORRETTO - API SONAR)
==========================================================

DESCRIZIONE:
Questo script implementa la pipeline di traduzione Sign Language -> Text usando l'architettura SONAR.
L'obiettivo Ã¨ adattare (fine-tune) un encoder visivo per proiettare video di lingua dei segni
nello spazio di embedding multilingue di SONAR.

PIPELINE DETTAGLIATA:
---------------------
1.  **DATA LOADING (How2SignDataset)**:
    -   Carica i video processati (features `.npy` o `.pt`).
    -   Carica le traduzioni testuali corrispondenti dal manifest `.tsv`.
    -   Gestisce il padding delle sequenze video (lunghezze variabili) nel `collate_fn`.

2.  **MODEL ARCHITECTURE (SONARFineTuner)**:
    -   **Encoder (Trainable)**:
        -   Input: Features visive (es. SignHiera 768-dim).
        -   Pooling: Media temporale delle features (T, 768) -> (768).
        -   Projection: MLP a 2 strati (768 -> 512 -> 1024) che mappa nello spazio SONAR.
        -   Output: Embedding SONAR (1024-dim).
    -   **Text Embedder (Frozen)**:
        -   Modello SONAR ufficiale (pre-addestrato).
        -   Input: Testo ground truth (inglese).
        -   Output: Embedding SONAR target (1024-dim).
        -   Scopo: Fornire il "bersaglio" per l'addestramento dell'encoder.
    -   **Text Decoder (Frozen)**:
        -   Modello SONAR ufficiale (pre-addestrato).
        -   Input: Embedding SONAR (predetto o target).
        -   Output: Testo tradotto.
        -   Scopo: Valutazione (calcolo BLEU) e inferenza.

3.  **TRAINING LOOP (`train_epoch`)**:
    -   Per ogni batch:
        a.  L'encoder predice l'embedding dal video (`pred_embeddings`).
        b.  L'embedder calcola l'embedding dal testo reale (`target_embeddings`).
        c.  Calcolo della Loss (vedi sotto).
        d.  Backpropagation solo sull'Encoder (Embedder e Decoder sono congelati).

4.  **LOSS FUNCTION (Il cuore del training)**:
    -   Combina due obiettivi per allineare i vettori nello spazio SONAR:
        a.  **Cosine Loss**: `1 - cosine_similarity(pred, target)`
            -   Allinea la DIREZIONE dei vettori (il significato semantico).
        b.  **Magnitude Loss**: `MSE(norm(pred), norm(target))`
            -   Allinea la SCALA/LUNGHEZZA dei vettori (l'intensitÃ  del segnale).
            -   Fondamentale perchÃ© il decoder SONAR richiede una norma specifica (~32) per funzionare.

5.  **EVALUATION (`evaluate`)**:
    -   Genera traduzioni usando il Decoder SONAR reale.
    -   Calcola il punteggio BLEU confrontando le predizioni con i riferimenti.
    -   Esegue Sanity Check per verificare che il decoder funzioni correttamente.

STORIA DELLE MODIFICHE E FIX (PerchÃ© ora funziona):
---------------------------------------------------
1.  **FIX BLEU 0% (Magnitudo)**:
    -   Problema: Inizialmente normalizzavamo gli embedding a norma 1 (`F.normalize`).
    -   Causa: Il decoder SONAR si aspetta vettori con norma molto piÃ¹ alta (~32).
        Con norma 1, il segnale era troppo debole e il decoder generava output vuoti.
    -   Soluzione: Rimossa normalizzazione forzata nel `forward`.

2.  **FIX LOSS FUNCTION (Direzione + Scala)**:
    -   Problema: Usando solo Cosine Loss, il modello imparava la direzione ma non la scala.
    -   Soluzione: Introdotta una Loss Composta:
        `Loss = CosineLoss (Direzione) + 1.0 * MSELoss (Magnitudo)`
        Questo costringe il modello a imparare sia COSA dire (direzione) che QUANTO forte dirlo (scala).

3.  **FIX FEATURE DIMENSION**:
    -   Problema: Lo script usava `input_dim=256` (vecchie features), ma SignHiera produce 768.
    -   Soluzione: Aggiornato default a `input_dim=768` e reso parametrico.

4.  **DEBUGGING & SANITY CHECKS**:
    -   Aggiunto monitoraggio di `p_norm` (norma predizioni) vs `t_norm` (norma target).
    -   Aggiunto Sanity Check: decodifica dei target reali per verificare che il decoder funzioni.

Autore: GitHub Copilot & Ignazio Emanuele PiccichÃ¨
Data: 20 Novembre 2025
Status: âœ… FUNZIONANTE (BLEU > 0.39% in quick test, atteso >30% in full training)
"""

import argparse
import json
import os
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import sacrebleu

# SONAR package (sonar-space) - API CORRETTA!
try:
    from sonar.inference_pipelines.text import (
        TextToEmbeddingModelPipeline,
        EmbeddingToTextModelPipeline,
    )

    SONAR_AVAILABLE = True
    print("âœ… SONAR imported successfully (sonar-space package)")
except ImportError as e:
    print("âŒ ERROR: SONAR (sonar-space) not installed.")
    print(f"   Error: {e}")
    print("\nðŸ”§ SOLUZIONE:")
    print('   pip install "sonar-space>=0.5.0"')
    print(
        "   pip install fairseq2 --extra-index-url https://fair.pkg.atmeta.com/fairseq2/whl/pt2.9.0/cu126"
    )
    SONAR_AVAILABLE = False
except Exception as e:
    print(f"âŒ ERROR importing SONAR: {e}")
    print(f"   Tipo: {type(e).__name__}")
    SONAR_AVAILABLE = False


class How2SignDataset(Dataset):
    """Dataset per feature How2Sign + traduzioni"""

    def __init__(self, features_dir: str, manifest_path: str, max_samples: int = None):
        self.features_dir = Path(features_dir)

        # Carica manifest con traduzioni
        manifest = pd.read_csv(manifest_path, sep="\t")

        print(f"ðŸ“‚ Manifest caricato: {len(manifest)} samples totali")

        # FILTRA solo sample con features disponibili
        valid_samples = []
        for idx, row in manifest.iterrows():
            video_id = row["id"]
            # Controlla se esiste .npy o .pt
            npy_path = self.features_dir / f"{video_id}.npy"
            pt_path = self.features_dir / f"{video_id}.pt"

            if npy_path.exists() or pt_path.exists():
                valid_samples.append(row)

        self.manifest = pd.DataFrame(valid_samples)

        print(f"   âœ… Features trovate: {len(self.manifest)} samples")
        print(
            f"   âš ï¸  Features mancanti: {len(manifest) - len(self.manifest)} samples (saltati)"
        )

        if max_samples and len(self.manifest) > max_samples:
            self.manifest = self.manifest.head(max_samples)
            print(f"   ðŸ“Š Limitato a: {len(self.manifest)} samples")

    def __len__(self):
        return len(self.manifest)

    def __getitem__(self, idx):
        row = self.manifest.iloc[idx]
        video_id = row["id"]  # FIX: colonna si chiama 'id' non 'video_id'
        text = row["text"]

        # Carica features (.npy o .pt)
        feature_path = self.features_dir / f"{video_id}.npy"

        if feature_path.exists():
            features = np.load(feature_path)
            features = torch.from_numpy(features).float()
        else:
            # Prova con .pt
            feature_path = self.features_dir / f"{video_id}.pt"
            data = torch.load(feature_path, map_location="cpu")
            features = data["features"]

        return {"video_id": video_id, "features": features, "text": text}  # (300, 256)


def collate_fn(batch):
    """Collate function per batch con padding"""
    video_ids = [item["video_id"] for item in batch]
    texts = [item["text"] for item in batch]

    # Features hanno lunghezze diverse: serve PADDING!
    # Trova lunghezza massima nel batch
    max_len = max(item["features"].shape[0] for item in batch)
    feature_dim = batch[0]["features"].shape[1]  # 256

    # Crea tensor padded
    padded_features = torch.zeros(len(batch), max_len, feature_dim)
    lengths = []

    for i, item in enumerate(batch):
        feat = item["features"]
        length = feat.shape[0]
        padded_features[i, :length, :] = feat
        lengths.append(length)

    return {
        "video_ids": video_ids,
        "features": padded_features,  # (B, max_len, 256) - con padding
        "lengths": torch.tensor(lengths),  # lunghezze originali
        "texts": texts,
    }


class SONARFineTuner(nn.Module):
    """
    Modello per il Fine-Tuning di SONAR.

    Questa classe incapsula l'intera logica di adattamento:
    - L'Encoder visivo (che addestriamo).
    - I modelli SONAR originali (che usiamo come "oracolo" e decoder).

    Componenti Dettagliati:
    1.  **Encoder (Trainable)**:
        -   Prende in input features visive (es. SignHiera 768-dim).
        -   Le proietta nello spazio SONAR (1024-dim).
        -   Non applica normalizzazione forzata (impara la scala dai dati).
        -   Ãˆ l'unica parte che accumula gradienti e si aggiorna.

    2.  **Text Embedder (Frozen)**:
        -   Usato solo in training per calcolare i "Target Embeddings" dalle frasi di ground truth.
        -   Ci dice "dove dovrebbe trovarsi" il vettore di questo video nello spazio semantico.

    3.  **Text Decoder (Frozen)**:
        -   Usato in evaluation per generare testo dagli embeddings predetti.
        -   Richiede il pacchetto `sonar-space`.
        -   Ãˆ il giudice finale della qualitÃ  della traduzione.
    """

    def __init__(
        self,
        encoder_checkpoint: str,
        input_dim: int = 768,
        device: str = "cuda",
        freeze_decoder: bool = True,
    ):
        super().__init__()

        if not SONAR_AVAILABLE:
            raise ImportError(
                "SONAR (sonar-space) is required. Install with: pip install 'sonar-space>=0.5.0'"
            )

        self.device = device
        self.input_dim = input_dim

        # 1. Carica SONAR ASL Encoder (pre-trained o from scratch)
        if encoder_checkpoint and os.path.exists(encoder_checkpoint):
            print(f"\nðŸ“¥ Loading SONAR ASL Encoder from {encoder_checkpoint}...")
            encoder_state = torch.load(encoder_checkpoint, map_location=device)
            self.encoder = self._build_encoder_from_state(encoder_state)
        else:
            print(
                f"\nðŸ†• Initializing SONAR ASL Encoder from scratch (no checkpoint)..."
            )
            self.encoder = self._build_encoder_from_state(None)

        # L'encoder SONAR mappa (T, 256) â†’ (1024,) embedding
        # Architettura: Transformer encoder
        self.encoder.to(device)

        # Encoder Ã¨ ADDESTRABILE (fine-tuning)
        for param in self.encoder.parameters():
            param.requires_grad = True

        print(
            f"âœ… Encoder loaded: {sum(p.numel() for p in self.encoder.parameters()) / 1e6:.1f}M params"
        )

        # 2. Carica SONAR Text Pipelines (pre-trained)
        print(f"\nðŸ“¥ Loading SONAR Text Decoder from sonar-space...")

        # Text embedder (per calcolare target embeddings durante training)
        self.text_embedder = TextToEmbeddingModelPipeline(
            encoder="text_sonar_basic_encoder",
            tokenizer="text_sonar_basic_encoder",
            device=torch.device(device),
        )

        # Text decoder (per generare traduzioni durante evaluation)
        self.text_decoder = EmbeddingToTextModelPipeline(
            decoder="text_sonar_basic_decoder",
            tokenizer="text_sonar_basic_encoder",
            device=torch.device(device),
        )

        # Decoder Ã¨ CONGELATO (pre-trained)
        if freeze_decoder:
            # I modelli SONAR sono giÃ  frozen di default
            print(f"ðŸ”’ Decoder frozen (pre-trained)")
        else:
            print(f"âš ï¸ Decoder unfrozen mode not supported with SONAR pipelines")

        print(f"âœ… Model ready!")

    def _build_encoder_from_state(self, state_dict=None):
        """
        Ricostruisce l'encoder da state_dict (o inizializza da zero se None)

        NOTA: Questa Ã¨ una semplificazione. In produzione si userebbe
        direttamente la classe SONAR da fairseq2.
        """
        # Dimensioni encoder SONAR
        # input_dim = 256  # VECCHIO (Features base)
        # input_dim = 768  # NUOVO (SignHiera Features)
        hidden_dim = 512
        output_dim = 1024  # SONAR embedding

        print(
            f"ðŸ”§ Encoder Architecture: Input={self.input_dim} -> Hidden={hidden_dim} -> Output={output_dim}"
        )

        # Simple projection per ora (in produzione: full transformer)
        encoder = nn.Sequential(
            nn.Linear(self.input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, output_dim),
        )

        # Carica pesi se compatibili e se forniti
        if state_dict is not None:
            try:
                encoder.load_state_dict(state_dict, strict=False)
                print("âœ… Loaded pre-trained encoder weights")
            except Exception as e:
                print(f"âš ï¸ Could not load encoder weights: {e}")
                print("âš ï¸ Using random initialization (will train from scratch)")
        else:
            print("âœ… Using random initialization (training from scratch)")
            print("âš ï¸ Using random initialization (will train from scratch)")

        return encoder

    def forward(self, features, lengths=None):
        """
        Forward pass: Trasforma sequenze video in embeddings SONAR.

        Il flusso dei dati Ã¨:
        Video (T frames) -> Features (T, 768) -> Pooling -> (768) -> MLP -> Embedding (1024)

        Args:
            features: (B, T, 256/768) video features (possibilmente con padding)
            lengths: (B,) lunghezze originali (prima del padding)

        Returns:
            embeddings: (B, 1024) SONAR embeddings pronti per il decoder
        """
        # 1. Pooling Temporale (Average Pooling)
        # Riduciamo la sequenza temporale (T) a un singolo vettore per video.
        # Questo Ã¨ un approccio semplice ma efficace per SONAR che lavora su frasi intere.
        # Encoder: (B, T, D) â†’ (B, D)
        B, T, D = features.shape

        if lengths is not None:
            # Media solo sui frame reali (escludendo il padding zero)
            # Questo Ã¨ cruciale per non "diluire" il segnale con gli zeri del padding
            features_avg = torch.zeros(B, D, device=features.device)
            for i in range(B):
                length = lengths[i].item()
                # Evita divisione per zero
                if length > 0:
                    features_avg[i] = features[i, :length, :].mean(dim=0)
        else:
            # Fallback: media su tutto (include padding se presente, meno preciso)
            features_avg = features.mean(dim=1)  # (B, D)

        # 2. Proiezione nello Spazio SONAR
        # L'encoder (MLP) mappa le feature visive (D) nello spazio multilingue (1024)
        embeddings = self.encoder(features_avg)  # (B, 1024)

        # NOTA IMPORTANTE SU SCALA E NORMALIZZAZIONE:
        # Non applichiamo normalizzazione L2 qui (F.normalize).
        # Il decoder SONAR ha bisogno di vettori con una specifica magnitudo (norma ~32),
        # non vettori unitari (norma 1).
        # Se normalizzassimo qui, distruggeremmo l'informazione di "intensitÃ " che il decoder si aspetta.
        # Lasciamo che la rete impari la scala corretta tramite la Magnitude Loss.

        return embeddings

    def decode(self, embeddings, max_length=512):
        """
        Decodifica embeddings â†’ testo usando decoder SONAR pre-trained

        Args:
            embeddings: (B, 1024) SONAR embeddings (torch.Tensor)
            max_length: Lunghezza massima output

        Returns:
            texts: List[str] traduzioni
        """
        if not SONAR_AVAILABLE:
            raise ImportError("SONAR (sonar-space) required for decoding")

        # Usa decoder SONAR pre-trained
        with torch.no_grad():
            # SONAR decoder si aspetta torch.Tensor (non numpy!)
            # Assicurati che sia su CPU per compatibilitÃ 
            if embeddings.is_cuda:
                embeddings = embeddings.cpu()

            # SONAR API: predict(embeddings, target_lang, max_seq_len)
            # embeddings deve essere torch.Tensor, non numpy array
            texts = self.text_decoder.predict(
                embeddings,  # Passa direttamente il tensore PyTorch
                target_lang="eng_Latn",  # Inglese
                max_seq_len=max_length,
            )

        return texts

    def encode_texts(self, texts: List[str]):
        """
        Codifica testi â†’ embeddings usando text encoder SONAR

        Args:
            texts: List[str] testi da codificare

        Returns:
            embeddings: (B, 1024) SONAR embeddings (detached, safe for backward)
        """
        if not SONAR_AVAILABLE:
            raise ImportError("SONAR (sonar-space) required for text encoding")

        with torch.no_grad():
            # SONAR API: predict(sentences, source_lang)
            embeddings = self.text_embedder.predict(
                texts,
                source_lang="eng_Latn",  # Inglese
            )

            # Converti a torch tensor
            if isinstance(embeddings, np.ndarray):
                embeddings = torch.from_numpy(embeddings).float()

            # IMPORTANTE: clone() per evitare errore inference mode
            # Gli embeddings SONAR sono creati in inference mode,
            # ma servono per calcolare loss (che richiede gradients)
            embeddings = embeddings.to(self.device).clone().detach()

            return embeddings


def train_epoch(
    model: SONARFineTuner,
    dataloader: DataLoader,
    optimizer: torch.optim.Optimizer,
    device: str,
) -> float:
    """Training loop per un epoch"""
    model.train()
    total_loss = 0.0

    progress = tqdm(dataloader, desc="Training")

    for batch in progress:
        features = batch["features"].to(device)  # (B, max_len, 256) - con padding
        lengths = batch["lengths"].to(device)  # (B,) - lunghezze originali
        texts = batch["texts"]

        # Forward: features â†’ embeddings
        pred_embeddings = model(features, lengths=lengths)  # (B, 1024)

        # Calcola target embeddings dai testi usando SONAR text encoder
        target_embeddings = model.encode_texts(texts)  # (B, 1024)

        # -------------------------------------------------------------------------
        # CALCOLO DELLA LOSS (Funzione di Costo)
        # -------------------------------------------------------------------------
        # Usiamo una loss composta da due parti per gestire sia la direzione che la scala.
        # Questo Ã¨ stato il FIX CRUCIALE per far funzionare il modello.

        # 1. Cosine Loss (Direzione/Semantica)
        # Misura l'angolo tra i vettori. Vogliamo che puntino nella stessa direzione.
        # Ãˆ la parte piÃ¹ importante per la traduzione (il significato).
        # Formula: 1 - cos(theta)
        pred_norm = torch.nn.functional.normalize(pred_embeddings, p=2, dim=1)
        target_norm = torch.nn.functional.normalize(target_embeddings, p=2, dim=1)
        cosine_sim = (pred_norm * target_norm).sum(dim=1).mean()
        loss_cosine = 1.0 - cosine_sim

        # 2. Magnitude Loss (Scala/IntensitÃ )
        # Misura la lunghezza dei vettori. Il decoder SONAR si aspetta una certa "energia".
        # Se i vettori sono troppo piccoli (es. norm=1), il decoder potrebbe generare silenzio.
        # Formula: MSE( ||pred||, ||target|| )
        pred_mag = pred_embeddings.norm(p=2, dim=1)
        target_mag = target_embeddings.norm(p=2, dim=1)
        loss_mag = torch.nn.functional.mse_loss(pred_mag, target_mag)

        # Loss Totale
        # Diamo prioritÃ  alla semantica (loss_cosine) ma correggiamo la scala (loss_mag)
        # AUMENTATO PESO MAGNITUDE: 0.01 -> 1.0 per forzare l'adattamento della scala
        # Senza questo peso alto, il modello convergeva a norma 1 e BLEU 0.
        loss = loss_cosine + 1.0 * loss_mag

        # Backward
        optimizer.zero_grad()
        loss.backward()

        # FIX 3: GRADIENT MONITORING
        total_norm = 0.0
        for p in model.encoder.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
        total_norm = total_norm**0.5

        # Gradient clipping per stabilitÃ 
        torch.nn.utils.clip_grad_norm_(model.encoder.parameters(), max_norm=1.0)

        optimizer.step()

        total_loss += loss.item()
        # -------------------------------------------------------------------------
        # LOGGING METRICHE (Barra di progresso)
        # -------------------------------------------------------------------------
        # loss   : Loss totale (quella che viene minimizzata)
        # cos    : Componente Cosine Loss (1 - similaritÃ ). PiÃ¹ basso Ã¨ meglio.
        # mag    : Componente Magnitude Loss (errore di scala). PiÃ¹ basso Ã¨ meglio.
        # p_norm : Norma media delle predizioni (quanto sono "forti" i vettori generati)
        # t_norm : Norma media dei target (quanto sono "forti" i vettori SONAR reali)
        #          NOTA: p_norm dovrebbe avvicinarsi a t_norm durante il training.
        # sim    : Cosine Similarity pura (quanto sono allineati i vettori). PiÃ¹ alto Ã¨ meglio (max 1.0).
        progress.set_postfix(
            {
                "loss": f"{loss.item():.4f}",
                "cos": f"{loss_cosine.item():.3f}",
                "mag": f"{loss_mag.item():.3f}",
                "p_norm": f"{pred_mag.mean().item():.1f}",
                "t_norm": f"{target_mag.mean().item():.1f}",
                "sim": f"{cosine_sim.item():.3f}",
            }
        )

    return total_loss / len(dataloader)


@torch.no_grad()
def evaluate(
    model: SONARFineTuner, dataloader: DataLoader, device: str
) -> Tuple[float, List[Dict]]:
    """Evaluation: calcola BLEU usando decoder SONAR reale"""
    model.eval()

    predictions = []
    references = []
    samples = []

    progress = tqdm(dataloader, desc="Evaluating")

    for batch in progress:
        features = batch["features"].to(device)  # (B, max_len, 256) - con padding
        lengths = batch["lengths"].to(device)  # (B,) - lunghezze originali
        texts = batch["texts"]
        video_ids = batch["video_ids"]

        # Forward: features â†’ embeddings
        embeddings = model(features, lengths=lengths)  # (B, 1024)

        # Decode usando SONAR decoder
        pred_texts = model.decode(embeddings, max_length=512)

        # SANITY CHECK: Decodifica anche i target embeddings (ground truth)
        # Questo ci dice se il decoder funziona correttamente con input perfetti
        if len(predictions) == 0:  # Fallo solo per il primo batch
            target_embeddings = model.encode_texts(texts)
            target_texts = model.decode(target_embeddings, max_length=512)
            print(f"\n[SANITY CHECK] Ground Truth Decoding:")
            print(f"  Original: {texts[0]}")
            print(f"  Decoded:  {target_texts[0]}")
            print(f"  T-Norm:   {target_embeddings[0].norm().item():.4f}")

        for i, (video_id, pred, ref) in enumerate(zip(video_ids, pred_texts, texts)):
            predictions.append(pred)
            references.append(ref)

            samples.append({"video_id": video_id, "reference": ref, "prediction": pred})

            # DEBUG: Stampa primi 3 esempi per capire cosa genera
            if len(predictions) <= 3:
                print(f"\n[DEBUG] Ref:  {ref}")
                print(f"[DEBUG] Pred: {pred}")

    # Calcola BLEU
    bleu = sacrebleu.corpus_bleu(predictions, [references])

    return bleu.score, samples


def main():
    parser = argparse.ArgumentParser(
        description="SONAR Fine-Tuning (APPROCCIO CORRETTO)"
    )

    # Paths
    parser.add_argument(
        "--encoder_checkpoint",
        type=str,
        default=None,
        help="Path to SONAR encoder checkpoint (.pth). If None, trains from scratch.",
    )
    parser.add_argument(
        "--train_features", type=str, required=True, help="Directory con train features"
    )
    parser.add_argument(
        "--train_manifest", type=str, required=True, help="Train manifest (.tsv)"
    )
    parser.add_argument(
        "--val_features", type=str, required=True, help="Directory con val features"
    )
    parser.add_argument(
        "--val_manifest", type=str, required=True, help="Val manifest (.tsv)"
    )
    parser.add_argument(
        "--output_dir", type=str, default="checkpoints", help="Output directory"
    )

    # Training
    parser.add_argument("--batch_size", type=int, default=32)
    parser.add_argument("--epochs", type=int, default=50)
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=1e-5,
        help="Learning rate (basso per fine-tuning!)",
    )
    parser.add_argument(
        "--eval_every", type=int, default=5, help="Evaluate every N epochs"
    )
    parser.add_argument(
        "--save_every", type=int, default=10, help="Save checkpoint every N epochs"
    )
    parser.add_argument(
        "--max_samples",
        type=int,
        default=None,
        help="Max samples per split (for quick test)",
    )

    # Model
    parser.add_argument(
        "--input_dim",
        type=int,
        default=768,
        help="Input feature dimension (256 for old features, 768 for SignHiera)",
    )
    parser.add_argument(
        "--freeze_decoder",
        action="store_true",
        default=True,
        help="Freeze decoder (recommended!)",
    )
    parser.add_argument("--device", type=str, default="cuda")

    # Evaluation only
    parser.add_argument(
        "--eval_only", action="store_true", help="Only evaluate, no training"
    )

    args = parser.parse_args()

    # Setup
    os.makedirs(args.output_dir, exist_ok=True)
    device = args.device if torch.cuda.is_available() else "cpu"
    print(f"ðŸš€ Using device: {device}")

    # Dataset
    print("\nðŸ“‚ Loading datasets...")
    train_dataset = How2SignDataset(
        args.train_features, args.train_manifest, max_samples=args.max_samples
    )
    val_dataset = How2SignDataset(
        args.val_features, args.val_manifest, max_samples=args.max_samples
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=2,
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=2,
    )

    # Model
    print("\nðŸ”§ Building model...")
    model = SONARFineTuner(
        encoder_checkpoint=args.encoder_checkpoint,
        input_dim=args.input_dim,
        device=device,
        freeze_decoder=args.freeze_decoder,
    )

    # Optimizer
    optimizer = torch.optim.AdamW(
        [p for p in model.parameters() if p.requires_grad],
        lr=args.learning_rate,
        weight_decay=0.01,
    )

    # Evaluation only
    if args.eval_only:
        print("\nðŸ“Š Evaluation only mode...")
        bleu, samples = evaluate(model, val_loader, device)

        print(f"\nâœ… BLEU: {bleu:.2f}%")

        # Save results
        results_path = Path(args.output_dir) / "test_results.json"
        with open(results_path, "w") as f:
            json.dump({"bleu": bleu, "samples": samples[:20]}, f, indent=2)

        print(f"ðŸ’¾ Results saved to {results_path}")
        return

    # Training
    print(f"\nðŸš€ Starting fine-tuning for {args.epochs} epochs...")
    print(f"ðŸ“Š Train: {len(train_dataset)} samples")
    print(f"ðŸ“Š Val: {len(val_dataset)} samples")
    print(f"ðŸ“Š Batch size: {args.batch_size}")
    print(f"ðŸ“Š Learning rate: {args.learning_rate}")
    print(f"ðŸ”’ Decoder frozen: {args.freeze_decoder}")

    best_bleu = 0.0
    metrics_history = []

    for epoch in range(1, args.epochs + 1):
        print(f"\n{'=' * 60}")
        print(f"Epoch {epoch}/{args.epochs}")
        print(f"{'=' * 60}")

        # Train
        train_loss = train_epoch(model, train_loader, optimizer, device)
        print(f"ðŸ“‰ Train Loss: {train_loss:.4f}")

        # Evaluate
        if epoch % args.eval_every == 0 or epoch == args.epochs:
            bleu, samples = evaluate(model, val_loader, device)
            print(f"ðŸ“Š Val BLEU: {bleu:.2f}%")

            # FIX 5: EVALUATION METRICS ESTESE
            # Calcola cosine similarity media (1 - loss se usiamo cosine loss)
            val_cosine = 1.0 - train_loss if train_loss < 2.0 else 0.0

            # Save metrics
            metrics = {
                "epoch": epoch,
                "train_loss": train_loss,
                "val_bleu": bleu,
                "val_cosine_sim": val_cosine,  # NUOVO
                "samples": samples[:5],
            }
            metrics_history.append(metrics)

            # Print detailed BLEU stats (1-gram, 2-gram, etc.)
            # bleu is a BLEUScore object, printing it shows details
            print(f"ðŸ“Š Val BLEU Details: {bleu}")

            # Save predictions
            pred_path = Path(args.output_dir) / f"predictions_epoch{epoch:03d}.json"
            with open(pred_path, "w") as f:
                json.dump(
                    {"epoch": epoch, "bleu": bleu, "samples": samples}, f, indent=2
                )

            # Save metrics
            metrics_path = Path(args.output_dir) / f"metrics_epoch{epoch:03d}.json"
            with open(metrics_path, "w") as f:
                json.dump(metrics, f, indent=2)

            # Save best model
            if bleu > best_bleu:
                best_bleu = bleu
                best_path = Path(args.output_dir) / "best_encoder.pt"
                torch.save(model.encoder.state_dict(), best_path)
                print(f"ðŸ’¾ Best model saved (BLEU: {bleu:.2f}%)")

    # Final summary
    print(f"\n{'=' * 60}")
    print(f"âœ… Fine-Tuning completato!")
    print(f"{'=' * 60}")
    print(f"ðŸ“Š Best BLEU: {best_bleu:.2f}%")
    print(f"ðŸ’¾ Models saved in: {args.output_dir}")

    # Save final config
    config_path = Path(args.output_dir) / "config.json"
    with open(config_path, "w") as f:
        json.dump(vars(args), f, indent=2)


if __name__ == "__main__":
    main()
