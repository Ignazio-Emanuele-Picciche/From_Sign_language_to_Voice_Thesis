{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f38a062c",
   "metadata": {},
   "source": [
    "# 6. Estrazione dei Landmark per il Dataset ASLLRP con MediaPipe\n",
    "\n",
    "Questo notebook ha lo scopo di risolvere un problema fondamentale per l'integrazione del dataset ASLLRP nel nostro processo di training: la mancanza dei file di landmark (keypoint) in formato JSON.\n",
    "\n",
    "Il nostro `LandmarkDataset` si aspetta di trovare questi file pre-generati, ma il dataset ASLLRP fornisce solo i file video.\n",
    "\n",
    "## Obiettivo\n",
    "\n",
    "L'obiettivo di questo notebook è automatizzare il processo di estrazione dei landmark dai video di ASLLRP utilizzando **MediaPipe**. I passaggi che seguiremo sono:\n",
    "\n",
    "1.  **Configurazione dei Percorsi**: Definiremo i percorsi di input (video e file CSV) e di output (dove salvare i file JSON generati).\n",
    "2.  **Identificazione dei Video**: Leggeremo il file `asllrp_video_sentiment_data_0.25_without_golden.csv` per ottenere la lista esatta dei video da processare.\n",
    "3.  **Esecuzione di MediaPipe**: Per ogni video, utilizzeremo la libreria MediaPipe per analizzare il video e salvare i landmark (posa, viso, mani) frame per frame.\n",
    "4.  **Verifica dell'Output**: Alla fine del processo, avremo una nuova cartella contenente i landmark in formato JSON, pronta per essere utilizzata nella fase di addestramento.\n",
    "\n",
    "**Requisiti:**\n",
    "\n",
    "- **MediaPipe installato**: È necessario aver installato la libreria con `pip install mediapipe`.\n",
    "- **OpenCV installato**: Necessario per la lettura dei video (`pip install opencv-python`).\n",
    "\n",
    "Procediamo con l'implementazione.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1fba1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La directory base del progetto è: /Users/ignazioemanuelepicciche/Documents/TESI Magistrale UCBM/Improved_EmoSign_Thesis\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ottieni la directory principale del progetto in modo robusto\n",
    "# Questo notebook si trova in 'notebooks', quindi torniamo indietro di una directory\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "print(f\"La directory base del progetto è: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febef912",
   "metadata": {},
   "source": [
    "## 1. Configurazione dei Percorsi\n",
    "\n",
    "In questa sezione, definiamo tutti i percorsi necessari per l'elaborazione. Non sono richieste modifiche manuali in questa cella.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57435dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cartella video ASLLRP: /Users/ignazioemanuelepicciche/Documents/TESI Magistrale UCBM/Improved_EmoSign_Thesis/data/raw/ASLLRP/batch_utterance_video_v3_1\n",
      "File CSV ASLLRP: /Users/ignazioemanuelepicciche/Documents/TESI Magistrale UCBM/Improved_EmoSign_Thesis/data/processed/asllrp_video_sentiment_data_0.34_without_golden.csv\n",
      "Cartella output JSON: /Users/ignazioemanuelepicciche/Documents/TESI Magistrale UCBM/Improved_EmoSign_Thesis/data/raw/ASLLRP/mediapipe_output_0.34/json\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.34\n",
    "\n",
    "# --- PERCORSI DI INPUT ---\n",
    "\n",
    "\n",
    "# Percorso della cartella contenente i video di ASLLRP\n",
    "ASLLRP_VIDEOS_DIR = os.path.join(\n",
    "    BASE_DIR, \"data\", \"raw\", \"ASLLRP\", \"batch_utterance_video_v3_1\"\n",
    ")\n",
    "\n",
    "# Percorso del file CSV che elenca i video di ASLLRP da usare\n",
    "ASLLRP_CSV_FILE = os.path.join(\n",
    "    BASE_DIR,\n",
    "    \"data\",\n",
    "    \"processed\",\n",
    "    f\"asllrp_video_sentiment_data_{threshold}_without_golden.csv\",\n",
    ")\n",
    "\n",
    "\n",
    "# --- PERCORSI DI OUTPUT ---\n",
    "\n",
    "# Cartella principale dove verranno salvati i landmark estratti\n",
    "OUTPUT_JSON_DIR = os.path.join(\n",
    "    BASE_DIR, \"data\", \"raw\", \"ASLLRP\", f\"mediapipe_output_{threshold}\", \"json\"\n",
    ")\n",
    "\n",
    "\n",
    "# --- VERIFICA DEI PERCORSI ---\n",
    "os.makedirs(OUTPUT_JSON_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Cartella video ASLLRP: {ASLLRP_VIDEOS_DIR}\")\n",
    "print(f\"File CSV ASLLRP: {ASLLRP_CSV_FILE}\")\n",
    "print(f\"Cartella output JSON: {OUTPUT_JSON_DIR}\")\n",
    "\n",
    "# Verifica se la cartella dei video esiste\n",
    "if not os.path.exists(ASLLRP_VIDEOS_DIR):\n",
    "    print(\n",
    "        \"\\nATTENZIONE: Il percorso della cartella dei video di ASLLRP non sembra corretto. Controllalo prima di procedere.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7125f462",
   "metadata": {},
   "source": [
    "## 2. Caricamento e Analisi del File CSV\n",
    "\n",
    "Leggiamo il file CSV per ottenere la lista dei video che dobbiamo processare. Questo ci assicura di elaborare solo i file pertinenti al nostro dataset di training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef342e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trovati 630 video unici da processare.\n",
      "\n",
      "Esempio di nomi di file video:\n",
      "- 99370717.mp4\n",
      "- 99395897.mp4\n",
      "- 99454786.mp4\n",
      "- 83594638.mp4\n",
      "- 83719008.mp4\n"
     ]
    }
   ],
   "source": [
    "# Leggi il file CSV\n",
    "try:\n",
    "    df_asllrp = pd.read_csv(ASLLRP_CSV_FILE)\n",
    "    # Rinominiamo la colonna per coerenza, se necessario\n",
    "    if \"Utterance video filename\" in df_asllrp.columns:\n",
    "        df_asllrp = df_asllrp.rename(columns={\"Utterance video filename\": \"video_name\"})\n",
    "\n",
    "    video_files_to_process = df_asllrp[\"video_name\"].unique().tolist()\n",
    "    print(f\"Trovati {len(video_files_to_process)} video unici da processare.\")\n",
    "\n",
    "    # Visualizza i primi 5 nomi di file video\n",
    "    print(\"\\nEsempio di nomi di file video:\")\n",
    "    for video_name in video_files_to_process[:5]:\n",
    "        print(f\"- {video_name}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERRORE: File CSV non trovato al percorso: {ASLLRP_CSV_FILE}\")\n",
    "    video_files_to_process = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc518eb",
   "metadata": {},
   "source": [
    "## 3. Esecuzione di MediaPipe per l'Estrazione dei Landmark\n",
    "\n",
    "Questo è il cuore del notebook. Iteriamo su ogni video della lista e utilizziamo `mediapipe` per estrarre i landmark.\n",
    "\n",
    "**Configurazione di MediaPipe:**\n",
    "\n",
    "Utilizzeremo il modello olistico (`mp.solutions.holistic`), che è una soluzione potente e integrata per rilevare simultaneamente:\n",
    "\n",
    "- **Pose landmarks**: Punti chiave del corpo.\n",
    "- **Face landmarks**: Punti chiave del viso.\n",
    "- **Hand landmarks**: Punti chiave delle mani.\n",
    "\n",
    "Il processo può richiedere del tempo, ma MediaPipe è generalmente molto più veloce di OpenPose e non richiede una GPU potente. I landmark estratti verranno salvati in formato JSON, replicando la struttura di output di OpenPose per garantire la compatibilità con il nostro `LandmarkDataset`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccf55908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inizio del processo di estrazione dei landmark con MediaPipe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1759136495.473667  209697 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 90.5), renderer: Apple M3\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "Estrazione Landmark:   0%|          | 0/630 [00:00<?, ?it/s]W0000 00:00:1759136495.587989  262713 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1759136495.643980  262711 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1759136495.647438  262717 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1759136495.649193  262714 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1759136495.649621  262715 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1759136495.657964  262717 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1759136495.660967  262714 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1759136495.668849  262715 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1759136495.719975  262711 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "Estrazione Landmark: 100%|██████████| 630/630 [1:46:26<00:00, 10.14s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estrazione dei landmark completata!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def landmarks_to_dict(landmark_list, num_landmarks, num_dimensions):\n",
    "    \"\"\"Converte una lista di landmark di MediaPipe in un array piatto.\"\"\"\n",
    "    if landmark_list is None:\n",
    "        return [0.0] * (num_landmarks * num_dimensions)\n",
    "\n",
    "    coords = []\n",
    "    for landmark in landmark_list.landmark:\n",
    "        coords.extend([landmark.x, landmark.y, landmark.z][:num_dimensions])\n",
    "    return coords\n",
    "\n",
    "\n",
    "print(\"Inizio del processo di estrazione dei landmark con MediaPipe...\")\n",
    "\n",
    "# Inizializza il modello olistico di MediaPipe\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "with mp_holistic.Holistic(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5,\n",
    ") as holistic:\n",
    "\n",
    "    # Cicla su ogni file video da processare con una barra di avanzamento\n",
    "    for video_filename in tqdm(video_files_to_process, desc=\"Estrazione Landmark\"):\n",
    "\n",
    "        # Percorso completo del video di input\n",
    "        input_video_path = os.path.join(ASLLRP_VIDEOS_DIR, video_filename)\n",
    "\n",
    "        # Crea una sottocartella specifica per i JSON di questo video\n",
    "        output_json_video_dir = os.path.join(\n",
    "            OUTPUT_JSON_DIR, os.path.splitext(video_filename)[0]\n",
    "        )\n",
    "        os.makedirs(output_json_video_dir, exist_ok=True)\n",
    "\n",
    "        # Verifica se il video esiste\n",
    "        if not os.path.exists(input_video_path):\n",
    "            print(f\"Video non trovato: {input_video_path}. Salto.\")\n",
    "            continue\n",
    "\n",
    "        # Apri il video\n",
    "        cap = cv2.VideoCapture(input_video_path)\n",
    "        frame_idx = 0\n",
    "\n",
    "        while cap.isOpened():\n",
    "            success, image = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "\n",
    "            # Converte l'immagine da BGR a RGB\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Processa l'immagine con MediaPipe\n",
    "            results = holistic.process(image_rgb)\n",
    "\n",
    "            # Estrai i landmark e convertili in un formato simile a OpenPose\n",
    "            pose_landmarks = landmarks_to_dict(results.pose_landmarks, 33, 3)\n",
    "            face_landmarks = landmarks_to_dict(results.face_landmarks, 468, 3)\n",
    "            left_hand_landmarks = landmarks_to_dict(results.left_hand_landmarks, 21, 3)\n",
    "            right_hand_landmarks = landmarks_to_dict(\n",
    "                results.right_hand_landmarks, 21, 3\n",
    "            )\n",
    "\n",
    "            # Crea la struttura dati JSON compatibile\n",
    "            output_data = {\n",
    "                \"version\": 1.3,\n",
    "                \"people\": [\n",
    "                    {\n",
    "                        \"person_id\": [-1],\n",
    "                        \"pose_keypoints_2d\": pose_landmarks,  # Adattare se necessario\n",
    "                        \"face_keypoints_2d\": face_landmarks,  # Adattare se necessario\n",
    "                        \"hand_left_keypoints_2d\": left_hand_landmarks,\n",
    "                        \"hand_right_keypoints_2d\": right_hand_landmarks,\n",
    "                        \"pose_keypoints_3d\": [],\n",
    "                        \"face_keypoints_3d\": [],\n",
    "                        \"hand_left_keypoints_3d\": [],\n",
    "                        \"hand_right_keypoints_3d\": [],\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "\n",
    "            # Salva il file JSON per il frame corrente\n",
    "            output_filename = os.path.join(\n",
    "                output_json_video_dir,\n",
    "                f\"{os.path.splitext(video_filename)[0]}_{frame_idx:012d}_keypoints.json\",\n",
    "            )\n",
    "            with open(output_filename, \"w\") as f:\n",
    "                json.dump(output_data, f)\n",
    "\n",
    "            frame_idx += 1\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "print(\"\\nEstrazione dei landmark completata!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14001b6c",
   "metadata": {},
   "source": [
    "## 4. Passaggi Successivi\n",
    "\n",
    "Se l'esecuzione è andata a buon fine, ora dovresti avere la cartella `data/raw/ASLLRP/mediapipe_output/json` popolata con sottocartelle, una per ogni video processato, contenenti i file JSON dei landmark.\n",
    "\n",
    "A questo punto, dobbiamo fare in modo che il nostro `LandmarkDataset` sappia dove trovare questi nuovi dati. La soluzione migliore è modificare la classe `LandmarkDataset` per accettare una **lista di directory di landmark** invece di una sola.\n",
    "\n",
    "Modificheremo `src/data_pipeline/landmark_dataset.py` e `src/utils/training_utils.py` per supportare questa nuova configurazione.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
