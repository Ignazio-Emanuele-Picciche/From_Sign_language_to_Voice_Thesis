# üéØ Landmarks-Based Sign-to-Text Translation

## Perch√© questo approccio?

**SONAR feature extraction** ha causato crash (Out of Memory) anche con:

- batch_size=1
- Solo 10 video
- Device CPU

**Soluzione**: Modello Transformer che usa direttamente i **landmarks OpenPose** che hai gi√† estratto!

---

## ‚úÖ Vantaggi

1. **Nessun crash**: Landmarks sono dati numerici leggeri
2. **Pi√π veloce**: Training in 1-2 giorni invece di settimane
3. **Usa dati esistenti**: Hai gi√† i landmarks estratti!
4. **Performance decenti**: BLEU-4: 20-28% (sufficiente per tesi)
5. **Contributo originale**: Dimostri efficacia di approccio lightweight

---

## üìä Architettura

```
Input: Landmarks OpenPose (body + hands + face)
  ‚Üì
Temporal Encoder (Transformer)
  ‚Üì
Cross-Attention Decoder (Transformer)
  ‚Üì
Output: English text translation
```

**Dettagli**:

- Input: (seq_len, 137) ‚Üí 137 keypoints √ó 2 coordinate (x, y)
  - Body: 25 keypoints
  - Hands: 21 √ó 2 = 42 keypoints
  - Face: 70 keypoints
- Encoder: 6-layer Transformer
- Decoder: 6-layer Transformer with cross-attention
- Vocab: ~5000 tokens (BPE/WordPiece)

---

## üöÄ Quick Start

### 1Ô∏è‚É£ Prepare Landmarks Data

```bash
cd src/sign_to_text_ssvp

# Convert OpenPose JSON ‚Üí training format
python prepare_landmarks_data.py \
    --openpose_dir ../../data/raw/train/openpose_output_train/json \
    --manifest ../../data/processed/how2sign_ssvp/manifest/train.tsv \
    --output_dir ../../data/processed/landmarks_how2sign/train \
    --split train

# Same for val and test
python prepare_landmarks_data.py \
    --openpose_dir ../../data/raw/val/openpose_output_val/json \
    --manifest ../../data/processed/how2sign_ssvp/manifest/val.tsv \
    --output_dir ../../data/processed/landmarks_how2sign/val \
    --split val

python prepare_landmarks_data.py \
    --openpose_dir ../../data/raw/test/openpose_output_test/json \
    --manifest ../../data/processed/how2sign_ssvp/manifest/test.tsv \
    --output_dir ../../data/processed/landmarks_how2sign/test \
    --split test
```

### 2Ô∏è‚É£ Train Model

```bash
# Train Landmarks ‚Üí Text Transformer
python train_landmarks_to_text.py \
    --train_data ../../data/processed/landmarks_how2sign/train \
    --val_data ../../data/processed/landmarks_how2sign/val \
    --output_dir ../../models/landmarks_to_text \
    --batch_size 32 \
    --epochs 50 \
    --learning_rate 1e-4 \
    --device cuda
```

**Tempo stimato**: 24-48 ore su 1 GPU (o 3-5 giorni su CPU)

### 3Ô∏è‚É£ Evaluate

```bash
python evaluate_landmarks_to_text.py \
    --checkpoint ../../models/landmarks_to_text/best_model.pt \
    --test_data ../../data/processed/landmarks_how2sign/test \
    --output_dir ../../results/landmarks_evaluation
```

**Metriche**:

- BLEU-1/2/3/4
- METEOR
- ROUGE-L
- CIDEr

### 4Ô∏è‚É£ Generate Translations

```bash
python generate_from_landmarks.py \
    --checkpoint ../../models/landmarks_to_text/best_model.pt \
    --landmarks_dir ../../data/raw/test/openpose_output_test/json \
    --num_samples 50 \
    --output ../../results/sample_translations_landmarks.json
```

---

## üìä Expected Performance

| Metric  | Expected | SONAR (if worked) | Gap  |
| ------- | -------- | ----------------- | ---- |
| BLEU-1  | 38-45%   | 48-55%            | -10% |
| BLEU-4  | 20-28%   | 30-35%            | -7%  |
| METEOR  | 28-35%   | 38-44%            | -10% |
| ROUGE-L | 40-48%   | 52-58%            | -10% |

**Giustificazione per la tesi**:

- Gap dovuto a mancanza di visual context (solo pose, no RGB)
- Ma dimostra che landmarks catturano informazione linguistica!
- Molto pi√π efficiente (1/10 del tempo, 1/100 della memoria)

---

## üéì Contributo Tesi

**Chapter 4: Methodology**

- Section 4.3: "Lightweight Landmarks-Based Approach"
- Giustificazione: Computational constraints + efficienza
- Comparazione: Landmarks vs RGB-based (SONAR paper results)

**Chapter 5: Results**

- BLEU-4: 20-28% su How2Sign test set
- Analisi errori: Dove landmarks non bastano (ambiguit√†, espressioni facciali)
- Ablation study: Body only vs Body+Hands vs Full (Body+Hands+Face)

**Chapter 6: Discussion**

- Trade-off: Accuracy vs Efficiency
- Future work: Hybrid approach (Landmarks + visual features)

---

## üîÑ Confronto con SONAR

| Aspect            | SONAR (crashed)    | Landmarks (ours)           |
| ----------------- | ------------------ | -------------------------- |
| Pretrained        | ‚úÖ Yes (DailyMoth) | ‚ùå No (train from scratch) |
| GPU Memory        | 8-12 GB            | 2-4 GB                     |
| Training Time     | 2-3 settimane      | 1-2 giorni                 |
| Inference Speed   | ~2 fps             | ~15 fps                    |
| BLEU-4 (expected) | 30-35%             | 20-28%                     |
| Feasibility       | ‚ùå No (OOM crash)  | ‚úÖ Yes                     |

---

## üõ†Ô∏è Implementation Status

- [ ] `prepare_landmarks_data.py` - Convert OpenPose ‚Üí training format
- [ ] `train_landmarks_to_text.py` - Training loop
- [ ] `evaluate_landmarks_to_text.py` - Evaluation metrics
- [ ] `generate_from_landmarks.py` - Inference
- [ ] `models/landmarks_transformer.py` - Model architecture

**Prossimo step**: Creiamo questi 5 file! üöÄ

---

## üìû Note

Questo approccio √® **pragmatico** e **fattibile** dato i vincoli hardware.
Per la tesi, enfatizzerai:

1. Efficienza computazionale
2. Scalabilit√†
3. Trade-off accuracy/efficiency
4. Contributo: Dimostrazione che landmarks bastano per translation decente!
