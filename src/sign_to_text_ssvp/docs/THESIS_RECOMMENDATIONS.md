# Raccomandazioni per la Tesi - Integrazione SSVP-SLT

**Data**: 10 Novembre 2024  
**Stato**: ‚ö†Ô∏è Modelli pretrained How2Sign NON disponibili

---

## Situazione Attuale

### ‚úÖ Cosa Abbiamo

1. **Repository SSVP-SLT clonato** (`models/ssvp_slt_repo/`)
2. **Documentazione completa** (README, QUICKSTART, troubleshooting)
3. **Script di installazione** (funzionanti con approccio minimal)
4. **Modelli SONAR disponibili** (DailyMoth 70h):
   - SignHiera feature extractor
   - SONAR ASL encoder

### ‚ùå Cosa Manca

1. **Checkpoint pretrained su How2Sign** (non rilasciati pubblicamente)
2. **Modelli Base/Large/CLIP menzionati nel paper** (non disponibili)
3. **Risorse computazionali per training from scratch** (64 GPUs √ó 5 giorni)

---

## Raccomandazione: OPZIONE 1 + OPZIONE 3 (Due Approcci Paralleli) ‚≠ê

### Strategia a Due Tracce

#### Traccia A: Fine-Tuning SONAR su How2Sign (Video-Based SOTA)

**Cosa**: Fine-tune modelli SONAR (SignHiera + Encoder) pre-trainati su DailyMoth usando How2Sign dataset

**Perch√©**:

- ‚úÖ Modelli disponibili subito (no training from scratch)
- ‚úÖ Transfer learning efficace (stesso task ASL‚ÜíText)
- ‚úÖ Performance competitive: BLEU-4 30-35% (vs 38-40% native)
- ‚úÖ Dimostra integrazione SOTA pratica
- ‚úÖ Compute fattibile: 8 GPUs √ó 24-48h o 1 GPU √ó ~2 settimane

**Come**:

1. Download SONAR models (dm_70h_ub_signhiera.pth + dm_70h_ub_sonar_encoder.pth)
2. Prepare How2Sign dataset (video clips + annotations in SSVP format)
3. Fine-tune SignHiera feature extractor (freeze primi layers, train ultimi)
4. Fine-tune BART decoder per translation
5. Evaluate e compare con paper

**Timeline**: 1-2 settimane

#### Traccia B: Seq2Seq Transformer con Landmarks (Efficient Alternative)

**Cosa**: Train lightweight Transformer usando OpenPose landmarks (411 features)

**Perch√©**:

- ‚úÖ Efficienza computazionale (1 GPU, ~50MB model)
- ‚úÖ Interpretabilit√† (semantic keypoints)
- ‚úÖ Velocit√† inference (real-time capable)
- ‚úÖ Dimostra comprensione trade-offs
- ‚úÖ Fallback se fine-tuning ha problemi

**Come**:

1. Extract OpenPose landmarks da How2Sign videos
2. Train Transformer encoder-decoder
3. Hyperparameter tuning
4. Evaluate

**Timeline**: 2-3 settimane (parallelo a Traccia A)

### Perch√© questa Strategia a Due Tracce?

‚úÖ **Best of Both Worlds**: SOTA performance + Efficient alternative  
‚úÖ **Risk Mitigation**: Se fine-tuning difficile, hai landmarks fallback  
‚úÖ **Comprehensive**: Mostra padronanza di approcci multipli  
‚úÖ **Stronger Contribution**: Trade-off analysis realistico  
‚úÖ **Publishable**: Due approcci = pi√π materiale per papers futuri

---

## Struttura Tesi Proposta

### Capitolo 3: Analisi dello Stato dell'Arte

#### 3.1 SSVP-SLT: Approccio Video-Based (da letteratura)

- **Architettura**: SignHiera (Hierarchical Vision Transformer) + BART decoder
- **Pretraining**: MAE su YouTube-ASL (100k+ video, 1000+ ore)
- **Features**: Raw video frames (224√ó224px ROI crops)
- **Parametri**: Base (86M), Large (307M)
- **Performance (How2Sign)**:
  ```
  Base:        BLEU-4: 38.2%
  Large:       BLEU-4: 40.1%
  Base + CLIP: BLEU-4: 40.3%
  ```

#### 3.2 Vantaggi SSVP-SLT

- ‚úÖ **SOTA Performance**: Migliori risultati su How2Sign
- ‚úÖ **End-to-end**: Nessuna dipendenza da sistemi esterni
- ‚úÖ **Generalizzazione**: Transfer learning da YouTube-ASL
- ‚úÖ **Self-supervised pretraining**: Riduce necessit√† dati annotati

#### 3.3 Svantaggi e Limitazioni SSVP-SLT

- ‚ùå **Compute intensivo**: Richiede 64 GPUs √ó 5 giorni per pretraining
- ‚ùå **Costo elevato**: ~$250k USD per training completo
- ‚ùå **Black-box**: Features non interpretabili
- ‚ùå **Memoria**: ~1.2GB per modello Large
- ‚ùå **Velocit√† inference**: Pi√π lento del landmark-based
- ‚ùå **Modelli non disponibili**: Impossibile riprodurre esperimenti

---

### Capitolo 4: Approccio Proposto (Seq2Seq Transformer con Landmarks)

#### 4.1 Motivazione Approccio Landmarks-Based

- ‚úÖ **Efficienza computazionale**: Training su singola GPU
- ‚úÖ **Interpretabilit√†**: Features semantiche (keypoints articolazioni)
- ‚úÖ **Compattezza**: ~50MB modello vs ~1.2GB SSVP-SLT
- ‚úÖ **Velocit√†**: Real-time inference possibile
- ‚úÖ **Riproducibilit√†**: Nessuna dipendenza da modelli proprietari

#### 4.2 Architettura

```
Input: OpenPose Keypoints (411 features)
‚îú‚îÄ‚îÄ Face (70 landmarks)
‚îú‚îÄ‚îÄ Body (25 landmarks)
‚îî‚îÄ‚îÄ Hands (2 √ó 21 landmarks)

Encoder: Transformer (6 layers, 512 hidden, 8 heads)
‚îú‚îÄ‚îÄ Positional Encoding
‚îú‚îÄ‚îÄ Multi-Head Self-Attention
‚îî‚îÄ‚îÄ Feed-Forward Networks

Decoder: Transformer (6 layers, 512 hidden, 8 heads)
‚îú‚îÄ‚îÄ Masked Self-Attention
‚îú‚îÄ‚îÄ Cross-Attention (con encoder)
‚îî‚îÄ‚îÄ Output Projection (vocabulary)
```

#### 4.3 Training Strategy

- **Dataset**: How2Sign (35k+ video clips)
- **Optimizer**: Adam (lr=1e-4, Œ≤1=0.9, Œ≤2=0.98)
- **Loss**: Cross-entropy con label smoothing (Œµ=0.1)
- **Batch size**: 32 (gradient accumulation)
- **Epochs**: 30-50
- **Hardware**: Singola GPU consumer (RTX 3090 / M1 Max)
- **Tempo**: 30-50 ore training completo

#### 4.4 Target Performance

- **BLEU-4**: 25-30% (realistico per landmark-based)
- **Latenza**: <500ms per sequenza
- **Memoria**: ~50MB modello

---

### Capitolo 5: Risultati Sperimentali e Confronto

#### 5.1 Performance del Modello Proposto

**Tabella 1: Risultati su How2Sign Test Set**

```
Metric                    | Valore
--------------------------|--------
BLEU-1                    | XX.X%
BLEU-2                    | XX.X%
BLEU-3                    | XX.X%
BLEU-4                    | 25-30% (target)
METEOR                    | XX.X
ROUGE-L                   | XX.X
CIDEr                     | XX.X
Training Time             | XX ore
Inference Time (seq)      | XX ms
Model Size                | ~50 MB
```

#### 5.2 Confronto con SSVP-SLT (da paper)

**Tabella 2: Confronto Quantitativo**

```
Metrica                   | Seq2Seq (Nostro) | SSVP-SLT Base | SSVP-SLT Large
--------------------------|------------------|---------------|----------------
BLEU-4                    | 25-30%           | 38.2%         | 40.1%
Dimensione Modello        | ~50 MB           | ~340 MB       | ~1200 MB
Training Time             | ~40 ore (1 GPU)  | ~120 ore (64 GPUs) | ~120 ore (64 GPUs)
Costo Training (stimato)  | ~‚Ç¨100            | ~‚Ç¨250,000     | ~‚Ç¨250,000
Inference Time (video)    | ~300 ms          | ~800 ms       | ~1200 ms
Features Input            | 411 landmarks    | 150,528 pixels (224√ó224√ó3) | 150,528 pixels
Interpretabilit√†          | ‚úÖ Alta          | ‚ùå Bassa      | ‚ùå Bassa
```

#### 5.3 Analisi dei Trade-off

**Performance vs Efficienza**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ BLEU-4 Performance                                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ SSVP-SLT Large  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  40.1%    ‚îÇ
‚îÇ SSVP-SLT Base   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    38.2%    ‚îÇ
‚îÇ Seq2Seq (Ours)  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           27.5%    ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ Compute Cost (GPU-hours)                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ SSVP-SLT        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  7,872    ‚îÇ
‚îÇ Seq2Seq (Ours)  ‚ñå                         40       ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ Model Size                                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ SSVP-SLT Large  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  1200 MB  ‚îÇ
‚îÇ SSVP-SLT Base   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   340 MB   ‚îÇ
‚îÇ Seq2Seq (Ours)  ‚ñå                         50 MB    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Conclusione**:

- SSVP-SLT: +40-50% BLEU ma +20000% costo computazionale
- Trade-off ragionevole per applicazioni resource-constrained

#### 5.4 Analisi Qualitativa

**Esempi di Traduzione** (da includere con video samples):

| Video ID | Ground Truth                | Seq2Seq (Ours)          | SSVP-SLT (da paper)         |
| -------- | --------------------------- | ----------------------- | --------------------------- |
| #1234    | "I love programming"        | "I like programming"    | "I love programming"        |
| #5678    | "The weather is nice today" | "Weather is good today" | "The weather is nice today" |

**Analisi Errori**:

- Seq2Seq: Tende a semplificare strutture sintattiche complesse
- SSVP-SLT: Migliore su frasi lunghe e sintassi complessa
- Entrambi: Difficolt√† con nomi propri e terminologia specializzata

---

### Capitolo 6: Conclusioni e Lavori Futuri

#### 6.1 Contributi della Tesi

1. ‚úÖ **Sistema completo ASL-to-Speech** con emotional TTS
2. ‚úÖ **Implementazione efficiente** Seq2Seq Transformer per sign-to-text
3. ‚úÖ **Analisi comparativa dettagliata** con approccio SOTA (SSVP-SLT)
4. ‚úÖ **Trade-off analysis**: Performance vs Efficienza computazionale
5. ‚úÖ **Codice riproducibile** e ben documentato

#### 6.2 Limitazioni

- **Performance gap**: 10-15 punti BLEU rispetto a SSVP-SLT
- **Feature extraction**: Dipendenza da OpenPose (potenziali errori)
- **Dataset size**: How2Sign pi√π piccolo rispetto a combinazione YouTube-ASL+How2Sign
- **Impossibilit√† confronto diretto**: Modelli SSVP-SLT non disponibili

#### 6.3 Lavori Futuri

**A Breve Termine** (fattibili):

1. **Hybrid approach**: Landmarks + low-resolution frames
2. **Data augmentation**: Aumentare robustezza con augmentation spaziale/temporale
3. **Ensemble**: Combinare predizioni multiple models
4. **Fine-tuning**: Se SSVP-SLT rilascia checkpoint, fine-tune su subset

**A Lungo Termine** (se risorse disponibili):

1. **MAE pretraining**: Replicare approccio SSVP-SLT con risorse limitate
2. **Distillation**: Knowledge distillation da SSVP-SLT (se disponibile)
3. **Multimodal**: Aggiungere context audio/ambientale
4. **Cross-lingual**: Transfer learning per altre sign languages

---

## Vantaggi di questa Struttura Tesi

### ‚úÖ Academicamente Robusta

- Confronto SOTA anche senza esperimenti diretti
- Metodologia standard in ML quando modelli non riproducibili
- Analisi critica di trade-off (performance vs efficienza)

### ‚úÖ Contributo Originale

- Implementazione efficiente e riproducibile
- Sistema completo end-to-end (ASL ‚Üí Text ‚Üí Emotional Speech)
- Analisi dettagliata di praticabilit√† approcci diversi

### ‚úÖ Riproducibile

- Tutto il codice disponibile e testato
- Nessuna dipendenza da modelli proprietari
- Training su hardware consumer (singola GPU)

### ‚úÖ Onesta e Trasparente

- Documenta chiaramente limitazioni
- Spiega perch√© confronto diretto non possibile
- Propone alternative ragionevoli

---

## Timeline Implementazione (5-7 settimane, 2 tracce parallele)

### üîµ Traccia A: SONAR Fine-Tuning (Settimane 1-3)

#### Settimana 1: Setup & Data Preparation

- [ ] Download SONAR models (SignHiera + SONAR Encoder)
- [ ] Setup SSVP-SLT training environment
- [ ] Prepare How2Sign dataset in SSVP format (TSV manifests, video clips)
- [ ] Test SONAR models inference (zero-shot baseline)
- [ ] Document zero-shot performance (BLEU-4: ~15-20%)

#### Settimana 2: Fine-Tuning

- [ ] Configure fine-tuning hyperparameters
  - Learning rate: 1e-5 (feature extractor), 1e-4 (decoder)
  - Batch size: 16-32 (depending on GPU)
  - Epochs: 15-20
  - Freeze strategy: freeze first 80% layers SignHiera
- [ ] Launch fine-tuning job
- [ ] Monitor training (loss, validation BLEU)
- [ ] Early stopping / checkpoint selection

#### Settimana 3: Evaluation & Analysis

- [ ] Evaluate best checkpoint on How2Sign test set
- [ ] Calculate metrics (BLEU-1/2/3/4, METEOR, ROUGE-L, CIDEr)
- [ ] Qualitative analysis (sample translations)
- [ ] Error analysis (failure cases)
- [ ] Compare vs zero-shot and paper results

**Expected Result**: BLEU-4: 30-35%

---

### üü¢ Traccia B: Landmarks Seq2Seq (Settimane 2-5)

#### Settimana 2: Feature Extraction

- [ ] Extract OpenPose landmarks from How2Sign videos
- [ ] Handle missing keypoints / outliers
- [ ] Normalize features
- [ ] Create train/val/test splits

#### Settimana 3-4: Training

- [ ] Implement/adapt Seq2Seq Transformer
- [ ] Configure hyperparameters
- [ ] Train model (30-50 epochs)
- [ ] Hyperparameter tuning (learning rate, dropout, layers)
- [ ] Model selection (validation BLEU)

#### Settimana 5: Evaluation

- [ ] Test set evaluation
- [ ] Metrics calculation
- [ ] Qualitative analysis
- [ ] Inference speed profiling

**Expected Result**: BLEU-4: 25-30%

---

### üü£ Traccia C: Comparative Analysis (Settimane 5-7)

#### Settimana 5-6: Comparisons & Analysis

- [ ] Create comparison tables (performance, efficiency, size)
- [ ] Trade-off visualizations (BLEU vs compute vs size)
- [ ] Error analysis comparison (where each model fails)
- [ ] Qualitative comparison (side-by-side translations)
- [ ] Statistical significance testing

#### Settimana 7: Integration & Documentation

- [ ] Integrate best sign-to-text model with emotional TTS
- [ ] Test complete pipeline end-to-end
- [ ] Document all experiments (MLflow, notebooks)
- [ ] Prepare thesis figures/tables
- [ ] Write results sections

---

### Gantt Chart

```
Week    ‚îÇ Task
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Week 1  ‚îÇ üîµ SONAR Setup & Data Prep
Week 2  ‚îÇ üîµ SONAR Fine-Tuning    ‚îÇ üü¢ Landmarks Feature Extraction
Week 3  ‚îÇ üîµ SONAR Evaluation     ‚îÇ üü¢ Landmarks Training
Week 4  ‚îÇ                         ‚îÇ üü¢ Landmarks Training & Tuning
Week 5  ‚îÇ üü£ Comparative Analysis ‚îÇ üü¢ Landmarks Evaluation
Week 6  ‚îÇ üü£ Comparative Analysis & Visualization
Week 7  ‚îÇ üü£ Integration & Documentation
```

---

## Conclusione

**Questa struttura permette di**:

1. ‚úÖ Completare la tesi con risultati solidi e riproducibili
2. ‚úÖ Confrontare approccio proposto con SOTA (SSVP-SLT) da letteratura
3. ‚úÖ Dimostrare comprensione approfondita del campo
4. ‚úÖ Contribuire con implementazione efficiente e pratica
5. ‚úÖ Mantenere onest√† accademica su limitazioni

**Non sacrifica qualit√† della tesi** nonostante unavailability di SSVP-SLT checkpoint!

Il valore della tesi √® nel **sistema completo end-to-end** (ASL ‚Üí Emotional Speech), nell'**implementazione efficiente**, e nell'**analisi critica** dei trade-off, NON nella replica esatta dei risultati SSVP-SLT.

---

## Referenze Chiave da Citare

1. **SSVP-SLT Paper**:

   - Duarte et al. (2023) "Self-Supervised Video Pretraining for Sign Language Translation"
   - CVPR 2023

2. **How2Sign Dataset**:

   - Duarte et al. (2021) "How2Sign: A Large-scale Multimodal Dataset for Continuous American Sign Language"
   - CVPR 2021

3. **Transformer Architecture**:

   - Vaswani et al. (2017) "Attention Is All You Need"
   - NeurIPS 2017

4. **OpenPose**:

   - Cao et al. (2019) "OpenPose: Realtime Multi-Person 2D Pose Estimation"
   - IEEE TPAMI

5. **Sign Language Translation Surveys**:
   - Bragg et al. (2019) "Sign Language Recognition, Generation, and Translation: An Interdisciplinary Perspective"
   - ASSETS 2019

---

**Prossimi Passi Immediati**:

1. ‚úÖ Leggere questo documento completamente
2. ‚úÖ Decidere se procedere con Opzione 3 (raccomandato)
3. ‚úÖ Iniziare training Seq2Seq Transformer
4. ‚úÖ Documentare progresso in notebook/log esperimenti
