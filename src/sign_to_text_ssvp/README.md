# ğŸ¬ SSVP-SLT Integration: Sign-to-Text con Video Pretraining

Integrazione del modello **SSVP-SLT** (Self-Supervised Video Pretraining for Sign Language Translation) di Facebook Research per traduzione da ASL a testo su **How2Sign**.

**Paper**: [Towards Privacy-Aware Sign Language Translation at Scale](https://arxiv.org/abs/2402.09611)  
**Repository**: [facebookresearch/ssvp_slt](https://github.com/facebookresearch/ssvp_slt)

---

## ğŸ“‹ Indice

1. [Overview](#-overview)
2. [Architettura](#-architettura)
3. [Differenze con Seq2Seq Transformer](#-differenze-con-seq2seq-transformer)
4. [Setup e Installazione](#-setup-e-installazione)
5. [Download Modelli Pretrained](#-download-modelli-pretrained)
6. [Preparazione Dataset How2Sign](#-preparazione-dataset-how2sign)
7. [Fine-tuning](#-fine-tuning)
8. [Evaluation](#-evaluation)
9. [Comparazione Modelli](#-comparazione-modelli)
10. [Performance Attese](#-performance-attese)

---

## ğŸ¯ Overview

### Cos'Ã¨ SSVP-SLT?

SSVP-SLT Ã¨ un approccio **state-of-the-art** per la traduzione da lingua dei segni a testo che utilizza:

1. **Masked Autoencoding (MAE)** su video per pretraining self-supervised
2. **Vision Transformer** per feature extraction end-to-end
3. **Fine-tuning** su dataset annotati (How2Sign)
4. **Privacy-preserving**: Lavora su video blurred/anonymized

### PerchÃ© SSVP-SLT?

| Aspetto                | Tuo Seq2Seq Transformer      | SSVP-SLT                          |
| ---------------------- | ---------------------------- | --------------------------------- |
| **Input**              | Landmarks OpenPose (411)     | Video RGB frames                  |
| **Pretraining**        | âŒ Training from scratch     | âœ… Self-supervised MAE            |
| **Architettura**       | Seq2Seq Transformer          | Vision Transformer + Decoder      |
| **BLEU-4 su How2Sign** | ~25-30% (target)             | **40%+** (SOTA)                   |
| **Robustezza**         | Dipende da qualitÃ  landmarks | End-to-end, piÃ¹ robusto           |
| **ScalabilitÃ **        | Richiede annotazioni         | Pretraining su video non annotati |

### Pipeline SSVP-SLT

```
1. Pretraining Self-Supervised (una tantum)
   Video non annotati â†’ MAE â†’ Learned representations

2. Fine-tuning Supervised (How2Sign)
   Video + caption pairs â†’ Fine-tune encoder + decoder â†’ Sign-to-text model

3. Inference
   Video ASL â†’ SSVP-SLT model â†’ Caption text
```

---

## ğŸ—ï¸ Architettura

### SSVP-SLT Model

```
Input: Video Frames [B, T, H, W, C]
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Video Preprocessing                â”‚
â”‚  - Resize: 224Ã—224                  â”‚
â”‚  - Normalize: ImageNet stats        â”‚
â”‚  - Temporal sampling: 16-32 frames  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vision Transformer Encoder (MAE)   â”‚
â”‚  - Patch embedding (16Ã—16)          â”‚
â”‚  - Positional encoding (spatial+temp)â”‚
â”‚  - Multi-head self-attention        â”‚
â”‚  - Pretrained on 70h+ video         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Contextualized Video Embeddings [B, T', D]
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Transformer Decoder                â”‚
â”‚  - Token embeddings                 â”‚
â”‚  - Cross-attention to video         â”‚
â”‚  - Causal self-attention            â”‚
â”‚  - Autoregressive generation        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Output: Caption Tokens [B, L]
    â†“
Decode â†’ "Hello my name is John"
```

### Componenti Principali

#### 1. **MAE Pretraining** (Masked Autoencoding)

- **Obiettivo**: Impara rappresentazioni video robuste
- **Metodo**: Maschera patch random (75%) e ricostruisce
- **Dataset**: DailyMoth-70h (70+ ore video ASL)
- **Output**: Encoder pretrained con feature generiche

#### 2. **Vision Transformer Encoder**

| Parametro    | Valore                    |
| ------------ | ------------------------- |
| Architecture | ViT-B/16 o ViT-L/16       |
| Input size   | 224Ã—224 pixels            |
| Patch size   | 16Ã—16                     |
| Layers       | 12 (Base) / 24 (Large)    |
| Hidden dim   | 768 (Base) / 1024 (Large) |
| Heads        | 12 (Base) / 16 (Large)    |

#### 3. **Transformer Decoder**

- **Architettura**: Standard Transformer decoder
- **Vocab size**: ~5000 tokens (BPE)
- **Max length**: 128 tokens
- **Decoding**: Beam search (beam=5)

---

## ğŸ”„ Differenze con Seq2Seq Transformer

### Approccio Input

**Tuo Modello (Landmarks-based)**:

```python
Video â†’ OpenPose â†’ Landmarks [T, 411] â†’ Seq2Seq â†’ Text
```

**SSVP-SLT (Video-based)**:

```python
Video â†’ Video frames [T, 224, 224, 3] â†’ ViT â†’ Text
```

### Pro/Contro

| Aspetto              | Landmarks (Tuo)                    | Video (SSVP-SLT)              |
| -------------------- | ---------------------------------- | ----------------------------- |
| **Preprocessing**    | âŒ Richiede OpenPose               | âœ… Solo resize/normalize      |
| **Robustezza**       | âš ï¸ Dipende da landmark quality     | âœ… End-to-end learning        |
| **InterpretabilitÃ ** | âœ… Feature esplicite (pose, hands) | âš ï¸ Black-box                  |
| **Performance**      | ğŸŸ¡ 25-30% BLEU                     | âœ… 40%+ BLEU                  |
| **Compute**          | âœ… PiÃ¹ leggero (411 features)      | âŒ PiÃ¹ pesante (immagini)     |
| **Pretraining**      | âŒ Training from scratch           | âœ… Pretrained representations |

### Quando Usare Quale?

**Usa il tuo Seq2Seq Transformer se**:

- Hai limitazioni computazionali
- Vuoi interpretabilitÃ  (analisi pose/hands/face)
- Hai giÃ  landmarks estratti
- Vuoi integrazione con analisi posturale

**Usa SSVP-SLT se**:

- Vuoi performance state-of-the-art
- Hai GPU potenti (V100/A100)
- Hai video grezzi disponibili
- Vuoi robustezza a occlusioni/rumore

---

## ğŸš€ Setup e Installazione

### Prerequisiti

```bash
# Python 3.8+
python --version

# CUDA 11.8+ (per GPU)
nvcc --version

# ffmpeg per video processing
brew install ffmpeg  # macOS
# o
sudo apt-get install ffmpeg  # Linux
```

### Installazione SSVP-SLT

```bash
# 1. Esegui script di setup automatico
cd src/sign_to_text_ssvp
bash scripts/install_ssvp.sh

# O manualmente:

# 2. Clona repository SSVP-SLT
git clone https://github.com/facebookresearch/ssvp_slt.git models/ssvp_slt_repo

# 3. Installa dipendenze SSVP-SLT
cd models/ssvp_slt_repo
pip install -r requirements.txt
pip install -e .

# 4. Installa fairseq (dependency)
cd fairseq-sl
pip install -e .

# 5. Torna alla root
cd ../../..
```

### Verifica Installazione

```bash
# Test import
python -c "import ssvp_slt; print('âœ… SSVP-SLT installed')"

# Check CUDA
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
```

---

## ğŸ“¥ Download Modelli Pretrained

### Modelli Disponibili

| Modello            | Size        | Pretraining    | BLEU-4 (How2Sign) | Download                                         |
| ------------------ | ----------- | -------------- | ----------------- | ------------------------------------------------ |
| **SSVP-Base**      | 86M params  | MAE (70h)      | ~38%              | [Link](https://dl.fbaipublicfiles.com/ssvp_slt/) |
| **SSVP-Large**     | 307M params | MAE (70h)      | **40%+**          | [Link](https://dl.fbaipublicfiles.com/ssvp_slt/) |
| **SSVP-Base-CLIP** | 86M params  | MAE+CLIP (70h) | ~39%              | [Link](https://dl.fbaipublicfiles.com/ssvp_slt/) |

### Download Automatico

```bash
# Download modello Base (consigliato per iniziare)
python download_pretrained.py --model base --output models/checkpoints/

# Download modello Large (per performance massime)
python download_pretrained.py --model large --output models/checkpoints/

# Download tutti i modelli
python download_pretrained.py --model all --output models/checkpoints/
```

### Download Manuale

Se lo script automatico fallisce:

```bash
# 1. Crea directory
mkdir -p models/checkpoints

# 2. Download da URL (esempio)
wget https://dl.fbaipublicfiles.com/ssvp_slt/how2sign/ssvp_base.pt \
     -O models/checkpoints/ssvp_base.pt

# 3. Verifica checksum
md5sum models/checkpoints/ssvp_base.pt
```

---

## ğŸ“Š Preparazione Dataset How2Sign

### Formato Richiesto SSVP-SLT

SSVP-SLT si aspetta video in formato specifico:

```
data/how2sign_ssvp/
â”œâ”€â”€ clips/
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ video_001.mp4
â”‚   â”‚   â”œâ”€â”€ video_002.mp4
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ val/
â”‚   â””â”€â”€ test/
â””â”€â”€ manifest/
    â”œâ”€â”€ train.tsv
    â”œâ”€â”€ val.tsv
    â””â”€â”€ test.tsv
```

### TSV Manifest Format

```tsv
id	duration	text
video_001	5.2	hello my name is john
video_002	12.8	how to cook pasta
```

### Conversione dal Tuo Dataset

```bash
# Script automatico di conversione
python prepare_how2sign_for_ssvp.py \
    --input_csv results/how2sign_splits/train_split.csv \
    --video_dir data/raw/how2sign/videos \
    --output_dir data/how2sign_ssvp \
    --split train

# Per tutti gli splits
bash scripts/prepare_all_splits.sh
```

### Script `prepare_how2sign_for_ssvp.py`

Il script fa:

1. âœ… Legge CSV con video_name, caption
2. âœ… Copia/symlink video in struttura SSVP-SLT
3. âœ… Genera TSV manifest files
4. âœ… Calcola durate video con ffprobe
5. âœ… Valida integritÃ  dataset

---

## ğŸ“ Fine-tuning

### Fine-tuning Rapido (Test)

```bash
# Quick test (3 epochs, subset dati)
python finetune_how2sign.py \
    --config configs/finetune_quick.yaml \
    --pretrained models/checkpoints/ssvp_base.pt \
    --data data/how2sign_ssvp \
    --output results/ssvp_finetune_quick \
    --epochs 3 \
    --subset 1000
```

### Fine-tuning Completo

```bash
# Full fine-tuning (30 epochs, full dataset)
python finetune_how2sign.py \
    --config configs/finetune_base.yaml \
    --pretrained models/checkpoints/ssvp_base.pt \
    --data data/how2sign_ssvp \
    --output results/ssvp_finetune_full \
    --epochs 30 \
    --batch_size 16 \
    --lr 1e-4 \
    --device cuda
```

### Fine-tuning con Modello Large

```bash
# Large model (migliori performance, richiede piÃ¹ memoria)
python finetune_how2sign.py \
    --config configs/finetune_large.yaml \
    --pretrained models/checkpoints/ssvp_large.pt \
    --data data/how2sign_ssvp \
    --output results/ssvp_finetune_large \
    --epochs 30 \
    --batch_size 8 \
    --accumulation_steps 2 \
    --device cuda
```

### Parametri Chiave

| Parametro           | Default  | Descrizione                   |
| ------------------- | -------- | ----------------------------- |
| `--pretrained`      | Required | Path checkpoint pretrained    |
| `--data`            | Required | Directory dataset SSVP format |
| `--epochs`          | 30       | Numero epoche fine-tuning     |
| `--batch_size`      | 16       | Batch size (ridurre se OOM)   |
| `--lr`              | 1e-4     | Learning rate                 |
| `--warmup_steps`    | 500      | Steps warmup learning rate    |
| `--grad_clip`       | 1.0      | Gradient clipping             |
| `--label_smoothing` | 0.1      | Label smoothing               |
| `--beam_size`       | 5        | Beam search width (inference) |

### Monitoring Training

```bash
# Logs in tempo reale
tail -f results/ssvp_finetune_full/train.log

# TensorBoard
tensorboard --logdir results/ssvp_finetune_full/tensorboard
```

---

## ğŸ“Š Evaluation

### Evaluation su Validation Set

```bash
# Valuta modello fine-tuned
python evaluate_how2sign.py \
    --checkpoint results/ssvp_finetune_full/best_checkpoint.pt \
    --data data/how2sign_ssvp \
    --split val \
    --output results/evaluation_ssvp_val.json
```

### Evaluation su Test Set

```bash
# Test set evaluation
python evaluate_how2sign.py \
    --checkpoint results/ssvp_finetune_full/best_checkpoint.pt \
    --data data/how2sign_ssvp \
    --split test \
    --output results/evaluation_ssvp_test.json \
    --save_predictions results/predictions_ssvp_test.csv
```

### Metriche Calcolate

- **BLEU-1, BLEU-2, BLEU-3, BLEU-4**: N-gram overlap
- **ROUGE-L**: Longest common subsequence
- **METEOR**: Semantic matching
- **WER**: Word Error Rate
- **CER**: Character Error Rate

### Output Evaluation

```json
{
  "checkpoint": "results/ssvp_finetune_full/best_checkpoint.pt",
  "split": "val",
  "num_samples": 1739,
  "metrics": {
    "bleu_1": 52.3,
    "bleu_2": 45.7,
    "bleu_3": 42.1,
    "bleu_4": 39.8,
    "rouge_l": 48.2,
    "meteor": 41.5,
    "wer": 28.4,
    "cer": 15.2
  },
  "caption_stats": {
    "avg_pred_length": 16.8,
    "avg_ref_length": 17.0,
    "exact_match_rate": 5.2
  }
}
```

---

## ğŸ”¬ Comparazione Modelli

### Script Comparazione

```bash
# Confronta SSVP-SLT vs Seq2Seq Transformer
python compare_models.py \
    --ssvp_checkpoint results/ssvp_finetune_full/best_checkpoint.pt \
    --seq2seq_checkpoint ../sign_to_text/models/sign_to_text/how2sign/best_checkpoint.pt \
    --data data/how2sign_ssvp \
    --split val \
    --output results/model_comparison.json
```

### Comparison Report

```markdown
================================================================================
ğŸ“Š MODEL COMPARISON: SSVP-SLT vs Seq2Seq Transformer
================================================================================

Dataset: How2Sign Validation (1739 samples)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric â”‚ SSVP-SLT â”‚ Seq2Seq (Yours) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ BLEU-4 â”‚ 39.8% âœ… â”‚ 25.3% â”‚
â”‚ BLEU-1 â”‚ 52.3% â”‚ 42.1% â”‚
â”‚ WER â”‚ 28.4% âœ… â”‚ 45.2% â”‚
â”‚ CER â”‚ 15.2% âœ… â”‚ 28.7% â”‚
â”‚ ROUGE-L â”‚ 48.2% âœ… â”‚ 38.9% â”‚
â”‚ Inference Speed â”‚ 12 fps â”‚ 35 fps âœ… â”‚
â”‚ Model Size â”‚ 86M params â”‚ 42M params âœ… â”‚
â”‚ GPU Memory â”‚ 8.2 GB â”‚ 4.1 GB âœ… â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ† Winner: SSVP-SLT

- Better accuracy (+14.5 BLEU-4 points)
- More robust (video-based)
- State-of-the-art performance

ğŸ’¡ Seq2Seq Advantages:

- Faster inference (3x)
- Lower memory footprint (2x)
- Interpretable features (landmarks)
```

---

## ğŸ“ˆ Performance Attese

### BLEU-4 su How2Sign (Validation)

| Model          | Pretraining | BLEU-4     | Note            |
| -------------- | ----------- | ---------- | --------------- |
| **SSVP-Base**  | MAE (70h)   | **38-40%** | SOTA            |
| **SSVP-Large** | MAE (70h)   | **40-42%** | Best            |
| SSVP-Base-CLIP | MAE+CLIP    | 39-41%     | Multimodal      |
| Seq2Seq (tuo)  | None        | 25-30%     | Landmarks-based |
| LSTM Baseline  | None        | 18-22%     | Old approach    |

### Training Time

| Model         | Hardware    | Full Fine-tuning (30 epochs) |
| ------------- | ----------- | ---------------------------- |
| SSVP-Base     | V100 (16GB) | ~24 ore                      |
| SSVP-Base     | A100 (40GB) | ~12 ore                      |
| SSVP-Large    | A100 (40GB) | ~36 ore                      |
| Seq2Seq (tuo) | M1/M2 MPS   | ~9 ore                       |

### Requisiti Hardware

**Minimum** (fine-tuning Base):

- GPU: 16GB VRAM (RTX 4090, V100)
- RAM: 32GB
- Storage: 100GB

**Recommended** (fine-tuning Large):

- GPU: 40GB VRAM (A100)
- RAM: 64GB
- Storage: 200GB

---

## ğŸ¯ Use Cases

### 1. **Benchmark SOTA**

Usa SSVP-SLT come **upper bound** per confrontare performance del tuo modello.

### 2. **Production Deployment**

Se serve massima accuratezza, usa SSVP-SLT in produzione.

### 3. **Ensemble Model**

Combina predizioni SSVP-SLT + Seq2Seq per robustezza:

```python
prediction = 0.7 * ssvp_output + 0.3 * seq2seq_output
```

### 4. **Thesis Comparison**

Mostra nella tesi:

- Approccio landmarks (tuo) vs video-based (SSVP-SLT)
- Trade-off accuracy vs efficienza
- Quando usare quale approccio

---

## ğŸ“š References

1. **Paper**: Rust et al. (2024) - "Towards Privacy-Aware Sign Language Translation at Scale"

   - [ACL Anthology](https://aclanthology.org/2024.acl-long.467/)
   - [arXiv](https://arxiv.org/abs/2402.09611)

2. **Repository**: [facebookresearch/ssvp_slt](https://github.com/facebookresearch/ssvp_slt)

3. **DailyMoth-70h Dataset**: 70+ ore video ASL per pretraining

4. **How2Sign**: Duarte et al. (2021) - Dataset principale per valutazione

---

## ğŸ“ TODO

- [ ] Setup iniziale e installazione SSVP-SLT
- [ ] Download modelli pretrained (Base + Large)
- [ ] Preparazione dataset How2Sign in formato SSVP
- [ ] Fine-tuning modello Base (quick test)
- [ ] Fine-tuning modello Base (full training)
- [ ] Evaluation su validation set
- [ ] Evaluation su test set
- [ ] Comparazione con Seq2Seq Transformer
- [ ] Documentazione risultati nella tesi

---

## ğŸ¤ Integrazione con Pipeline Completa

### Integrazione in `EmoSign Pipeline`

```python
# Pipeline attuale
Video ASL â†’ Landmarks â†’ Seq2Seq â†’ Text â†’ Emotion â†’ TTS

# Pipeline con SSVP-SLT
Video ASL â†’ SSVP-SLT â†’ Text â†’ Emotion â†’ TTS

# Pipeline Ensemble (best of both)
Video ASL â†’ [Landmarks â†’ Seq2Seq] â†’ Text
         â†˜ [SSVP-SLT]           â†— (ensemble)
                                â†“
                           Emotion â†’ TTS
```

---

## ğŸ“§ Support

Per problemi o domande:

- Vedi documentazione SSVP-SLT: [GitHub Issues](https://github.com/facebookresearch/ssvp_slt/issues)
- Consulta paper originale: [arXiv](https://arxiv.org/abs/2402.09611)

---

## âš–ï¸ License

- **SSVP-SLT**: CC-BY-NC 4.0 (non-commercial use)
- **Tua integrazione**: Segui license del tuo progetto

**Note**: SSVP-SLT Ã¨ per uso di ricerca. Per uso commerciale, contatta Facebook Research.
