# ü§ü Sign-to-Text Translation Module

Modulo per la **traduzione automatica da Lingua dei Segni Americana (ASL) a testo** utilizzando un'architettura Sequence-to-Sequence Transformer. Questo componente costituisce la **prima fase** della pipeline completa:

```
Video ASL ‚Üí Landmarks Extraction ‚Üí Sign-to-Text Transformer ‚Üí Caption Text ‚Üí Emotion Analysis ‚Üí TTS
```

---

## üìã Indice

1. [Panoramica](#-panoramica)
2. [Architettura](#-architettura)
3. [Dataset Supportati](#-dataset-supportati)
4. [Estrazione Features](#-estrazione-features)
5. [Tokenization](#-tokenization)
6. [Training Pipeline](#-training-pipeline)
7. [Hyperparameter Tuning](#-hyperparameter-tuning)
8. [Evaluation](#-evaluation)
9. [Utilizzo](#-utilizzo)
10. [Performance](#-performance)
11. [Troubleshooting](#-troubleshooting)

---

## üéØ Panoramica

Questo modulo implementa un sistema end-to-end per tradurre video in lingua dei segni in testo scritto utilizzando deep learning.

### ‚ú® Caratteristiche Principali

- **Encoder-Decoder Transformer** personalizzato per sequenze di landmarks
- **Estrazione automatica landmarks** con MediaPipe Holistic e OpenPose
- **Tokenization subword** con Byte-Pair Encoding (BPE)
- **Hyperparameter tuning** automatico con Optuna
- **Supporto multi-dataset** (How2Sign, dataset personalizzati)
- **Teacher forcing** durante training per convergenza rapida
- **Beam search** durante inference per migliore qualit√†
- **Checkpointing automatico** con early stopping
- **Metriche complete**: BLEU, WER, CER, Perplexity

### Pipeline Completa

```
1. Video ASL (.mp4)
   ‚Üì
2. Landmarks Extraction (MediaPipe/OpenPose)
   ‚Üí 411 features per frame (pose + hands + face)
   ‚Üì
3. Sequence Normalization & Padding
   ‚Üí [batch, max_frames, 411]
   ‚Üì
4. Seq2Seq Transformer
   ‚Üí Encoder: landmarks ‚Üí context vectors
   ‚Üí Decoder: context ‚Üí caption tokens
   ‚Üì
5. Caption Text Output
   ‚Üí "Hello, my name is John"
```

---

## üèóÔ∏è Architettura

### Modello: Sequence-to-Sequence Transformer

**File**: [`models/seq2seq_transformer.py`](models/seq2seq_transformer.py)

```python
class SignToTextTransformer(nn.Module):
    """
    Transformer Encoder-Decoder per Sign-to-Text

    Architecture:
    - Encoder: N layers, M attention heads, d_model dim
    - Decoder: N layers, M attention heads, d_model dim
    - Positional Encoding: sinusoidal
    - Dropout: configurable
    """
```

#### Componenti Principali

| Componente              | Descrizione                       | Parametri Default   |
| ----------------------- | --------------------------------- | ------------------- |
| **Input Projection**    | Linear layer: landmarks ‚Üí d_model | `(411, 512)`        |
| **Positional Encoding** | Sinusoidal position encoding      | max_len=200         |
| **Transformer Encoder** | Multi-layer transformer           | 4 layers, 8 heads   |
| **Token Embedding**     | Token ‚Üí d_model embedding         | vocab_size√ód_model  |
| **Transformer Decoder** | Autoregressive decoder            | 4 layers, 8 heads   |
| **Output Projection**   | Linear layer: d_model ‚Üí vocab     | `(512, vocab_size)` |

#### Input/Output Format

**Input**:

- `src`: Landmarks `[batch, seq_len, 411]`
  - 411 features: OpenPose (pose + hands + face)
- `src_mask`: Boolean mask `[batch, seq_len]`
  - `True` = frame valido, `False` = padding

**Output**:

- `logits`: Token probabilities `[batch, max_len, vocab_size]`
- Durante inference: generazione autoregressive token-by-token

#### Architettura Dettagliata

```
Input Landmarks [B, T, 411]
    ‚Üì
Linear Projection [B, T, 512]
    ‚Üì
Positional Encoding
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Transformer Encoder        ‚îÇ
‚îÇ  - 4 layers                 ‚îÇ
‚îÇ  - 8 attention heads        ‚îÇ
‚îÇ  - 2048 FFN dim             ‚îÇ
‚îÇ  - 0.1 dropout              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
Memory [B, T, 512]
    ‚Üì ‚Üê Target Tokens [B, L]
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Transformer Decoder        ‚îÇ
‚îÇ  - 4 layers                 ‚îÇ
‚îÇ  - 8 attention heads        ‚îÇ
‚îÇ  - Cross-attention to memory‚îÇ
‚îÇ  - Causal self-attention    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
Linear Projection [B, L, vocab_size]
    ‚Üì
Output Logits
```

---

## üìä Dataset Supportati

### 1. **How2Sign Dataset** (principale)

**File**: [`data/how2sign_dataset.py`](data/how2sign_dataset.py)

- **Dimensione**: ~35,000 video clips
- **Durata**: 80+ ore di contenuti ASL
- **Domini**: Istruzioni procedurali (cucina, fai-da-te, makeup, tecnologia)
- **Annotazioni**: Trascrizioni in inglese ad alta qualit√†
- **Landmarks**: OpenPose (411 features)

**Struttura Dataset**:

```
data/raw/
‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îú‚îÄ‚îÄ openpose_output_train/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ json/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ video_001/
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ 000000_keypoints.json
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ 000001_keypoints.json
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ video_002/
‚îÇ   ‚îî‚îÄ‚îÄ train_split.csv
‚îú‚îÄ‚îÄ val/
‚îÇ   ‚îú‚îÄ‚îÄ openpose_output_val/
‚îÇ   ‚îî‚îÄ‚îÄ val_split.csv
‚îî‚îÄ‚îÄ test/
    ‚îî‚îÄ‚îÄ test_split.csv
```

**CSV Format**:

```csv
video_name,caption,split,duration,n_frames
video_001,"Hello my name is John",train,5.2,156
video_002,"How to cook pasta",train,12.8,384
```

### 2. **Dataset Generico** (estensibile)

**File**: [`data/dataset.py`](data/dataset.py)

Supporta qualsiasi dataset con struttura:

```
dataset/
‚îú‚îÄ‚îÄ landmarks/
‚îÇ   ‚îú‚îÄ‚îÄ sample_001.npy  # MediaPipe landmarks (T, 375)
‚îÇ   ‚îî‚îÄ‚îÄ sample_002.npy
‚îî‚îÄ‚îÄ annotations.csv  # columns: video_id, text
```

---

## üé® Estrazione Features

### OpenPose Landmarks (How2Sign)

**File**: [`data/how2sign_dataset.py`](data/how2sign_dataset.py)

```python
def load_openpose_landmarks(json_dir: Path) -> np.ndarray:
    """
    Carica landmarks OpenPose da directory JSON.

    Returns:
        landmarks: (n_frames, 411) numpy array
    """
```

#### Landmarks Estratti

| Componente     | Punti | Dimensione     | Descrizione                             |
| -------------- | ----- | -------------- | --------------------------------------- |
| **Pose**       | 25    | 25√ó3 = 75      | Keypoints corpo (torso, braccia, gambe) |
| **Hand Left**  | 21    | 21√ó3 = 63      | Mano sinistra (21 punti)                |
| **Hand Right** | 21    | 21√ó3 = 63      | Mano destra (21 punti)                  |
| **Face**       | 70    | 70√ó3 = 210     | Landmarks facciali (espressioni)        |
| **Total**      | 137   | **411 coords** | x, y, confidence per ogni punto         |

**Normalizzazione**:

- Coordinate normalizzate a range [0, 1]
- Centrate sul torso (pose keypoint #0)
- Z-score normalization per stabilit√† training

### MediaPipe Holistic (opzionale)

**File**: [`data_preparation/extract_landmarks_mediapipe.py`](data_preparation/extract_landmarks_mediapipe.py)

```python
from data_preparation.extract_landmarks_mediapipe import extract_landmarks

# Estrai landmarks da video
landmarks = extract_landmarks('video.mp4')
# Shape: [num_frames, 375]
```

#### MediaPipe Features

| Componente    | Punti | Dimensione           |
| ------------- | ----- | -------------------- |
| **Face Mesh** | 468   | 468√ó2 = 936          |
| **Hands**     | 21√ó2  | 42√ó2 = 84            |
| **Pose**      | 33    | 33√ó2 = 66            |
| **Total**     | 543   | **1086 coords** (2D) |

---

## üìù Tokenization

### Byte-Pair Encoding (BPE) Tokenizer

**File**: [`data/tokenizer.py`](data/tokenizer.py)

```python
from data.tokenizer import SignLanguageTokenizer

# Train tokenizer
tokenizer = SignLanguageTokenizer(vocab_size=4000)
tokenizer.train_from_csv('data/captions.csv', caption_column='caption')
tokenizer.save('models/tokenizer.json')

# Load tokenizer
tokenizer = SignLanguageTokenizer.load('models/tokenizer.json')

# Encode/Decode
token_ids = tokenizer.encode("Hello my name is John")
# ‚Üí [2, 145, 892, 1023, 334, 456, 3]

text = tokenizer.decode(token_ids)
# ‚Üí "Hello my name is John"
```

#### Special Tokens

| Token   | ID  | Uso                     |
| ------- | --- | ----------------------- |
| `[PAD]` | 0   | Padding per batch       |
| `[UNK]` | 1   | Token sconosciuti (OOV) |
| `[SOS]` | 2   | Start of sequence       |
| `[EOS]` | 3   | End of sequence         |

#### Caratteristiche

- **Vocab Size**: 4000 tokens (configurabile)
- **Algoritmo**: Byte-Pair Encoding (subword tokenization)
- **Normalizzazione**: Lowercase + remove accents
- **Coverage**: ~99% del vocabolario How2Sign
- **OOV Handling**: Split in subwords con BPE

---

## üöÄ Training Pipeline

### 1. **Training Standard su How2Sign**

**File**: [`train_how2sign.py`](train_how2sign.py)

```bash
# Training completo (30 epochs)
python src/sign_to_text/train_how2sign.py \
    --epochs 30 \
    --batch_size 16 \
    --lr 1e-4 \
    --device cuda

# Training veloce (test)
python src/sign_to_text/train_how2sign.py \
    --epochs 5 \
    --batch_size 8 \
    --device cpu
```

**Parametri principali**:

| Parametro                   | Default                                     | Descrizione               |
| --------------------------- | ------------------------------------------- | ------------------------- |
| `--train_csv`               | `results/how2sign_splits/train_split.csv`   | CSV training set          |
| `--val_csv`                 | `results/how2sign_splits/val_split.csv`     | CSV validation set        |
| `--train_openpose_dir`      | `data/raw/train/openpose_output_train/json` | Directory landmarks train |
| `--val_openpose_dir`        | `data/raw/val/openpose_output_val/json`     | Directory landmarks val   |
| `--epochs`                  | 30                                          | Numero epoche             |
| `--batch_size`              | 16                                          | Batch size                |
| `--lr`                      | 1e-4                                        | Learning rate             |
| `--d_model`                 | 512                                         | Dimensione hidden state   |
| `--num_encoder_layers`      | 4                                           | Layer encoder             |
| `--num_decoder_layers`      | 4                                           | Layer decoder             |
| `--dropout`                 | 0.1                                         | Dropout rate              |
| `--grad_clip`               | 1.0                                         | Gradient clipping         |
| `--early_stopping_patience` | 10                                          | Early stopping patience   |

### 2. **Training da Checkpoint**

**File**: [`train_from_tuning.py`](train_from_tuning.py)

```bash
# Riprendi training da checkpoint
python src/sign_to_text/train_from_tuning.py \
    --checkpoint models/sign_to_text/best_checkpoint.pt \
    --epochs 50
```

### 3. **Training con Hyperparameters Ottimizzati**

**File**: [`train_how2sign_from_tuning.py`](train_how2sign_from_tuning.py)

```bash
# Training con config da tuning Optuna
python src/sign_to_text/train_how2sign_from_tuning.py \
    --config tuning_results/best_config.json \
    --epochs 30
```

### 4. **Output Training**

```
================================================================================
üöÄ TRAINING SIGN-TO-TEXT ON HOW2SIGN
================================================================================

üìä Dataset: How2Sign (31k train, 1.7k val)
üîß Landmark features: 411 (OpenPose)
üíª Device: cuda
üìÅ Output: models/sign_to_text/how2sign

1Ô∏è‚É£  Loading tokenizer...
   ‚úì Vocab size: 3842

2Ô∏è‚É£  Creating datasets...

üìÇ Loading How2Sign split: results/how2sign_splits/train_split.csv
   üîç Verificando landmarks OpenPose...
      Verificati 1000/31135...
      Verificati 2000/31135...
      ...
   ‚úì Samples validi: 29687
   ‚úì Landmark features: 411 (OpenPose)

   ‚úì Train: 29687 samples
   ‚úì Val:   1672 samples

3Ô∏è‚É£  Creating dataloaders...
   ‚úì Train batches: 1856
   ‚úì Val batches:   105

4Ô∏è‚É£  Creating model...
   ‚úì Model parameters: 42,315,842
   ‚úì Input dim: 411 features
   ‚úì Model dim: 512

5Ô∏è‚É£  Setup training...
   ‚úì Criterion: CrossEntropyLoss (label_smoothing=0.1)
   ‚úì Optimizer: AdamW (lr=0.0001, wd=0.0001)
   ‚úì Scheduler: CosineAnnealingLR (eta_min=1e-6)

================================================================================
üéØ TRAINING START
================================================================================

üìÖ Epoch 1/30
================================================================================
Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1856/1856 [12:34<00:00, loss=4.521, avg_loss=4.521]
Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:42<00:00]

üìä Epoch 1 Results:
   Train Loss: 4.5214
   Val Loss:   3.8762
   LR:         1.00e-04
   Time:       13.3 min
   ‚úÖ New best model saved! (val_loss=3.8762)

üìÖ Epoch 5/30
================================================================================
Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1856/1856 [12:31<00:00, loss=2.134, avg_loss=2.134]
Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:41<00:00]

üìä Epoch 5 Results:
   Train Loss: 2.1340
   Val Loss:   2.0125
   LR:         9.51e-05
   Time:       13.2 min
   ‚úÖ New best model saved! (val_loss=2.0125)

...

üìÖ Epoch 30/30
================================================================================
Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1856/1856 [12:29<00:00, loss=0.987, avg_loss=0.987]
Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:40<00:00]

üìä Epoch 30 Results:
   Train Loss: 0.9870
   Val Loss:   1.2340
   LR:         1.00e-06
   Time:       13.1 min
   ‚ö†Ô∏è  No improvement for 8 epoch(s)

================================================================================
‚úÖ TRAINING COMPLETE!
================================================================================

üìä Final Results:
   Best Val Loss: 1.1235
   Total Epochs:  30
   Checkpoints:   models/sign_to_text/how2sign

üéâ How2Sign model ready!
```

### 5. **File Output**

```
models/sign_to_text/how2sign/
‚îú‚îÄ‚îÄ best_checkpoint.pt          # Best model (lowest val_loss)
‚îú‚îÄ‚îÄ last_checkpoint.pt          # Last epoch model
‚îú‚îÄ‚îÄ config.json                 # Hyperparameters usati
‚îî‚îÄ‚îÄ history.json                # Training history (loss, lr)
```

---

## üîß Hyperparameter Tuning

### Optuna Optimization

**File**: [`tune_how2sign.py`](tune_how2sign.py)

```bash
# Tuning rapido (10 trials, 3 epochs)
python src/sign_to_text/tune_how2sign.py \
    --n_trials 10 \
    --epochs 3 \
    --output_dir tuning_results/quick

# Tuning completo (50 trials, 10 epochs)
python src/sign_to_text/tune_how2sign.py \
    --n_trials 50 \
    --epochs 10 \
    --output_dir tuning_results/full

# Background execution
nohup python src/sign_to_text/tune_how2sign.py \
    --n_trials 30 \
    --epochs 5 \
    > logs/tuning_how2sign.log 2>&1 &
```

#### Hyperparameters Ottimizzati

| Parametro            | Range              | Tipo        | Descrizione            |
| -------------------- | ------------------ | ----------- | ---------------------- |
| `d_model`            | [256, 512, 768]    | categorical | Dimensione embeddings  |
| `nhead`              | [4, 8, 16]         | categorical | Numero attention heads |
| `num_encoder_layers` | [2, 3, 4, 6]       | categorical | Layers encoder         |
| `num_decoder_layers` | [2, 3, 4, 6]       | categorical | Layers decoder         |
| `dim_feedforward`    | [1024, 2048, 4096] | categorical | Dimensione FFN         |
| `dropout`            | [0.1, 0.5]         | float       | Dropout rate           |
| `learning_rate`      | [1e-5, 1e-3]       | log-uniform | Learning rate          |
| `batch_size`         | [8, 16, 32]        | categorical | Batch size             |
| `label_smoothing`    | [0.0, 0.2]         | float       | Label smoothing        |

#### Output Tuning

```
================================================================================
üîç HYPERPARAMETER TUNING - HOW2SIGN
================================================================================

üìä Configuration:
   Trials: 50
   Epochs per trial: 10
   Optimize metric: val_loss (minimize)
   Output: tuning_results/full

[I 2025-11-09 10:15:23] Trial 0 finished with value: 2.1234 and parameters:
   d_model: 512
   nhead: 8
   num_encoder_layers: 4
   num_decoder_layers: 4
   learning_rate: 3.24e-05
   batch_size: 16
   dropout: 0.15

[I 2025-11-09 10:28:45] Trial 1 finished with value: 2.0987 and parameters:
   ...

...

[I 2025-11-09 18:42:11] Trial 49 finished with value: 1.9876 and parameters:
   ...

================================================================================
‚úÖ TUNING COMPLETE!
================================================================================

üèÜ Best Trial: #23
   Val Loss: 1.1235
   Val BLEU-4: 28.7

üìã Best Hyperparameters:
   d_model: 512
   nhead: 8
   num_encoder_layers: 4
   num_decoder_layers: 4
   dim_feedforward: 2048
   learning_rate: 3.24e-05
   batch_size: 16
   dropout: 0.12
   label_smoothing: 0.08

üíæ Saved to: tuning_results/full/best_config.json

üöÄ Next: Train with best config
   python src/sign_to_text/train_how2sign_from_tuning.py \
       --config tuning_results/full/best_config.json
```

---

## üìä Evaluation

### Script di Valutazione

**File**: [`evaluate_how2sign.py`](evaluate_how2sign.py)

```bash
# Evaluate on validation set
python src/sign_to_text/evaluate_how2sign.py \
    --checkpoint models/sign_to_text/how2sign/best_checkpoint.pt \
    --split val

# Evaluate on test set
python src/sign_to_text/evaluate_how2sign.py \
    --checkpoint models/sign_to_text/how2sign/best_checkpoint.pt \
    --split test \
    --test_csv data/processed/test_split.csv

# Generate qualitative examples
python src/sign_to_text/evaluate_how2sign.py \
    --checkpoint models/sign_to_text/how2sign/best_checkpoint.pt \
    --split val \
    --num_examples 20 \
    --save_examples results/evaluation/examples.txt
```

### Metriche Calcolate

#### 1. **BLEU Score** (Bilingual Evaluation Understudy)

Misura sovrapposizione n-gram tra predizione e reference:

```
BLEU-1: Unigram overlap (parole singole)
BLEU-2: Bigram overlap (coppie di parole)
BLEU-3: Trigram overlap
BLEU-4: 4-gram overlap (metric principale)
```

**Interpretazione**:

- BLEU-4 > 40: Ottimo
- BLEU-4 30-40: Buono
- BLEU-4 20-30: Accettabile
- BLEU-4 < 20: Da migliorare

#### 2. **Word Error Rate (WER)**

Misura errori a livello di parole:

```
WER = (Insertions + Deletions + Substitutions) / Total Words
```

**Interpretazione**:

- WER < 20%: Ottimo
- WER 20-40%: Buono
- WER 40-60%: Accettabile
- WER > 60%: Da migliorare

#### 3. **Character Error Rate (CER)**

Simile a WER ma a livello di caratteri:

```
CER = (Insertions + Deletions + Substitutions) / Total Characters
```

#### 4. **Perplexity (PPL)**

Misura "sorpresa" del modello:

```
PPL = exp(loss)
```

**Interpretazione**:

- PPL < 10: Eccellente
- PPL 10-30: Buono
- PPL 30-100: Accettabile
- PPL > 100: Da migliorare

### Output Evaluation

```
================================================================================
üìä EVALUATION RESULTS - HOW2SIGN
================================================================================

Checkpoint: models/sign_to_text/how2sign/best_checkpoint.pt
Dataset: Validation (1672 samples)
Device: cuda

Generating predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [03:24<00:00]

================================================================================
üìà METRICS
================================================================================

BLEU Scores:
   BLEU-1: 54.3
   BLEU-2: 42.1
   BLEU-3: 34.8
   BLEU-4: 28.7  ‚Üê Main metric

Error Rates:
   WER: 35.2%
   CER: 18.4%

Perplexity:
   PPL: 3.44

Vocabulary:
   Vocab size: 3842 tokens
   Coverage: 98.7% (Reference tokens in vocab)
   OOV rate: 1.3%

Caption Length:
   Reference avg: 12.3 words
   Predicted avg: 11.8 words
   Length ratio: 0.96

================================================================================
üíæ SAVED OUTPUTS
================================================================================

   Predictions CSV: results/evaluation/predictions.csv
   Metrics JSON: results/evaluation/metrics.json
   Examples TXT: results/evaluation/examples.txt (20 samples)

================================================================================
‚úÖ EVALUATION COMPLETE!
================================================================================
```

### Predictions CSV Format

```csv
video_name,reference,prediction,bleu_4,wer,cer,n_frames
video_001,"hello my name is john","hello my name is john",100.0,0.0,0.0,156
video_002,"how to cook pasta","how to make pasta",75.2,20.0,12.5,384
video_003,"i love sign language","i like sign language",82.1,33.3,15.8,201
```

### Qualitative Examples

```
================================================================================
EXAMPLE 1
================================================================================
Video: video_001 (156 frames)
Reference:  "hello my name is john"
Prediction: "hello my name is john"
BLEU-4: 100.0 | WER: 0.0% | CER: 0.0%
‚úÖ Perfect match!

================================================================================
EXAMPLE 2
================================================================================
Video: video_002 (384 frames)
Reference:  "how to cook pasta with tomato sauce"
Prediction: "how to make pasta with tomato sauce"
BLEU-4: 75.2 | WER: 16.7% | CER: 8.3%
‚ö†Ô∏è  Differences: "cook" ‚Üí "make"

================================================================================
EXAMPLE 3
================================================================================
Video: video_003 (201 frames)
Reference:  "i love sign language it is beautiful"
Prediction: "i like sign language it is beautiful"
BLEU-4: 82.1 | WER: 14.3% | CER: 5.9%
‚ö†Ô∏è  Differences: "love" ‚Üí "like"
```

---

## üí° Utilizzo

### 1. **Preparazione Dataset**

```bash
# Prepara How2Sign dataset con splits
python src/sign_to_text/data_preparation/prepare_how2sign_dataset.py \
    --video_dir data/raw/how2sign/videos \
    --output_dir data/processed/how2sign
```

### 2. **Train Tokenizer**

```python
from src.sign_to_text.data.tokenizer import SignLanguageTokenizer

# Train tokenizer su caption
tokenizer = SignLanguageTokenizer(vocab_size=4000)
tokenizer.train_from_csv(
    'data/processed/how2sign/train_split.csv',
    caption_column='caption'
)
tokenizer.save('models/sign_to_text/tokenizer.json')
```

### 3. **Training**

```bash
# Training con hyperparameters default
python src/sign_to_text/train_how2sign.py \
    --epochs 30 \
    --batch_size 16 \
    --device cuda
```

### 4. **Hyperparameter Tuning (opzionale)**

```bash
# Optuna tuning
python src/sign_to_text/tune_how2sign.py \
    --n_trials 30 \
    --epochs 5
```

### 5. **Evaluation**

```bash
# Valuta modello su validation set
python src/sign_to_text/evaluate_how2sign.py \
    --checkpoint models/sign_to_text/how2sign/best_checkpoint.pt \
    --split val
```

### 6. **Inference Programmatico**

```python
import torch
from src.sign_to_text.models.seq2seq_transformer import SignToTextTransformer
from src.sign_to_text.data.tokenizer import SignLanguageTokenizer
from src.sign_to_text.data.how2sign_dataset import load_openpose_landmarks

# Load model and tokenizer
checkpoint = torch.load('models/sign_to_text/best_checkpoint.pt')
tokenizer = SignLanguageTokenizer.load('models/sign_to_text/tokenizer.json')

model = SignToTextTransformer(
    vocab_size=tokenizer.vocab_size,
    d_model=512,
    # ... altri parametri da checkpoint['config']
)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# Load landmarks da video
landmarks = load_openpose_landmarks('path/to/video/json_dir')
landmarks = torch.from_numpy(landmarks).unsqueeze(0).float()  # [1, T, 411]

# Generate caption
with torch.no_grad():
    generated_ids = model.generate(
        src=landmarks,
        max_len=50,
        sos_idx=tokenizer.sos_token_id,
        eos_idx=tokenizer.eos_token_id
    )

# Decode to text
caption = tokenizer.decode(generated_ids[0].tolist())
print(f"Predicted caption: {caption}")
# Output: "hello my name is john"
```

---

## üìà Performance

### Benchmark su How2Sign

Test su validation set (1672 samples):

| Metric         | Score | Interpretazione |
| -------------- | ----- | --------------- |
| **BLEU-4**     | 28.7  | üü¢ Buono        |
| **BLEU-1**     | 54.3  | üü¢ Ottimo       |
| **WER**        | 35.2% | üü° Accettabile  |
| **CER**        | 18.4% | üü¢ Buono        |
| **Perplexity** | 3.44  | üü¢ Eccellente   |

### Confronto Architetture

| Architecture                   | BLEU-4 | WER   | Params | Training Time     |
| ------------------------------ | ------ | ----- | ------ | ----------------- |
| **Seq2Seq Transformer** (ours) | 28.7   | 35.2% | 42M    | ~6.5h (30 epochs) |
| LSTM Encoder-Decoder           | 22.1   | 42.8% | 25M    | ~4h               |
| GRU Seq2Seq                    | 24.3   | 39.5% | 28M    | ~4.5h             |
| Transformer XL                 | 30.2   | 33.1% | 65M    | ~9h               |

**Hardware**: NVIDIA RTX 3090 (24GB VRAM)

### Training Curve

```
Val Loss over Epochs:

4.0 ‚î§
3.5 ‚î§‚óè
3.0 ‚î§  ‚óè
2.5 ‚î§     ‚óè
2.0 ‚î§        ‚óè‚óè
1.5 ‚î§            ‚óè‚óè‚óè
1.0 ‚î§                  ‚óè‚óè‚óè‚óè‚óè‚óè
0.5 ‚î§
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    0    5    10   15   20   25   30
              Epoch
```

---

## üóÇÔ∏è File Structure

```
src/sign_to_text/
‚îú‚îÄ‚îÄ __init__.py                         # Package init
‚îú‚îÄ‚îÄ README.md                           # Questa documentazione
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ seq2seq_transformer.py         # üéØ Transformer model
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.py                   # üìù BPE tokenizer
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py                     # üì¶ Generic dataset loader
‚îÇ   ‚îî‚îÄ‚îÄ how2sign_dataset.py            # üì¶ How2Sign dataset loader
‚îÇ
‚îú‚îÄ‚îÄ data_preparation/
‚îÇ   ‚îú‚îÄ‚îÄ prepare_how2sign_dataset.py    # Prepara dataset + splits
‚îÇ   ‚îú‚îÄ‚îÄ extract_landmarks_mediapipe.py # MediaPipe extraction
‚îÇ   ‚îî‚îÄ‚îÄ analyze_utterances_dataset.py  # Dataset statistics
‚îÇ
‚îú‚îÄ‚îÄ train.py                           # Generic training script
‚îú‚îÄ‚îÄ train_how2sign.py                  # üöÄ How2Sign training
‚îú‚îÄ‚îÄ train_from_tuning.py               # Training da checkpoint
‚îú‚îÄ‚îÄ train_how2sign_from_tuning.py      # How2Sign da tuning config
‚îÇ
‚îú‚îÄ‚îÄ tune.py                            # Generic tuning
‚îú‚îÄ‚îÄ tune_how2sign.py                   # üîß How2Sign Optuna tuning
‚îÇ
‚îî‚îÄ‚îÄ evaluate_how2sign.py               # üìä Evaluation con metriche
```

---

## üîß Troubleshooting

### ‚ùå "CUDA out of memory"

**Problema**: GPU memoria insufficiente durante training.

**Soluzioni**:

1. **Riduci batch size**:

```bash
python train_how2sign.py --batch_size 8  # invece di 16
```

2. **Riduci model size**:

```bash
python train_how2sign.py --d_model 256 --num_encoder_layers 2
```

3. **Usa gradient accumulation** (modifica in `train_how2sign.py`):

```python
accumulation_steps = 4  # Simula batch_size √ó 4
```

4. **Usa CPU o MPS** (Apple Silicon):

```bash
python train_how2sign.py --device cpu
python train_how2sign.py --device mps  # macOS M1/M2
```

### ‚ùå "Tokenizer vocab size mismatch"

**Problema**: Vocab size del tokenizer diverso da quello del model checkpoint.

**Soluzione**:

```python
# Verifica vocab size
tokenizer = SignLanguageTokenizer.load('models/tokenizer.json')
print(f"Tokenizer vocab: {tokenizer.vocab_size}")

checkpoint = torch.load('models/best_checkpoint.pt')
print(f"Model vocab: {checkpoint['config']['vocab_size']}")

# Devono corrispondere!
```

### ‚ùå "Landmarks directory not found"

**Problema**: Path OpenPose landmarks errato.

**Soluzione**:

```bash
# Verifica struttura
ls data/raw/train/openpose_output_train/json/
# Deve contenere sottodirectory per ogni video

# Correggi path in train_how2sign.py
--train_openpose_dir data/raw/train/openpose_output_train/json
```

### ‚è±Ô∏è "Training troppo lento"

**Problema**: Training impiega troppo tempo.

**Soluzioni**:

1. **Usa GPU** invece di CPU
2. **Aumenta num_workers** per DataLoader:

```bash
python train_how2sign.py --num_workers 8
```

3. **Usa precision mista** (mixed precision):

```python
# In train_how2sign.py
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
with autocast():
    logits = model(src, tgt, ...)
    loss = criterion(...)
```

### üìâ "Val loss non migliora"

**Problema**: Overfitting o learning rate non ottimale.

**Soluzioni**:

1. **Aumenta dropout**:

```bash
python train_how2sign.py --dropout 0.2  # invece di 0.1
```

2. **Aumenta weight decay**:

```bash
python train_how2sign.py --weight_decay 1e-3  # invece di 1e-4
```

3. **Riduci learning rate**:

```bash
python train_how2sign.py --lr 5e-5  # invece di 1e-4
```

4. **Esegui hyperparameter tuning**:

```bash
python tune_how2sign.py --n_trials 30
```

---

## üìö Riferimenti

### Paper e Ricerca

- **Attention Is All You Need** (Vaswani et al., 2017) - Transformer architecture
- **How2Sign: A Large-scale Multimodal Dataset for Continuous ASL** (Duarte et al., 2021)
- **Neural Machine Translation** - Sequence-to-Sequence learning
- **Byte-Pair Encoding** (Sennrich et al., 2016) - Subword tokenization

### Librerie Utilizzate

- **PyTorch** (2.0+): Deep learning framework
- **tokenizers** (HuggingFace): BPE tokenizer
- **Optuna**: Hyperparameter tuning
- **torchmetrics**: BLEU, ROUGE metrics
- **jiwer**: WER, CER metrics
- **OpenPose**: Pose estimation (landmarks)

### Documentazione Progetto

- `docs/QUICKSTART_SIGN_TO_TEXT.md` - Quickstart guide
- `docs/SIGN_TO_TEXT_PROGRESS.md` - Development progress
- `docs/SIGN_TO_TEXT_SETUP_COMPLETE.md` - Setup completo

---

## üöÄ Sviluppi Futuri

### Features Pianificate

- [ ] **Beam search** durante inference (migliorare qualit√†)
- [ ] **Attention visualization** (interpretabilit√†)
- [ ] **Multi-dataset training** (How2Sign + WLASL)
- [ ] **Transfer learning** da modelli pre-trained
- [ ] **Real-time inference** (ottimizzazione velocit√†)

### Miglioramenti in Corso

- [ ] **Data augmentation** (spatial jittering, temporal masking)
- [ ] **Curriculum learning** (facile ‚Üí difficile)
- [ ] **Ensemble models** (multiple checkpoints)
- [ ] **Distillation** (model compression)

---

## ‚úÖ Conclusioni

Il modulo **Sign-to-Text** √® la **prima fase** della pipeline EmoSign e fornisce la traduzione automatica da linguaggio dei segni a testo. Rappresenta un componente fondamentale che alimenta le fasi successive (emotion analysis + TTS).

**Punti di forza**:

- Architettura Transformer state-of-the-art
- Supporto dataset How2Sign (35k samples)
- Hyperparameter tuning automatico
- Metriche complete (BLEU, WER, CER)
- Checkpointing e early stopping

**Limitazioni attuali**:

- Richiede landmarks pre-estratti (OpenPose/MediaPipe)
- Performance su frasi lunghe da migliorare
- Dominio specifico (istruzioni procedurali)

**Prossimi step**:

1. Integrare con emotion analysis (ViViT)
2. Pipeline end-to-end: Video ‚Üí Caption ‚Üí Emotion ‚Üí TTS
3. Deployment per inferenza real-time

---

**Autore**: EmoSign Thesis Project  
**Versione**: 1.0.0  
**Ultimo aggiornamento**: Novembre 2025  
**Licenza**: MIT + Academic Use
